{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### 使用fasttext工具实现word2vec的训练和使用\n",
    "\n",
    "- 第一步:获取训练数据\n",
    "- 第二步:训练词向量\n",
    "- 第三步:模型超参数设定\n",
    "- 第四步:模型效果检验\n",
    "- 第五步:模型的保存与重加载\n",
    "\n",
    "#### 第一步:获取训练数据\n",
    "\n",
    "-  wget -c http://mmattmahoney.net/dc/enwik9.zip -P data\n",
    "- unzip data/enwik9.zip -d data\n",
    "- Head -10 data/enwik9\n",
    "- Perl wikifil.pl data/enwik9 > data/fil9\n",
    "- Head -c 80 data/fil9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fileinput\n",
    "\n",
    "import fasttext\n",
    "# 使用fasttext的train_unsupervised(无监督训练方法)进行词向量的训练\n",
    "model = fasttext.train_unsupervised('data/fil9')\n",
    "\n",
    "# 有效训练词汇量为124M,共218316个单词\n",
    "\n",
    "# 通过get_word_vector方法来获得指定词汇的词向量\n",
    "model.get_word_vector(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 第三步：模型超参数的设定\n",
    "\n",
    "- 在训练词向量过程中，我们可以设定很多常用超参数来调节我们模型效果如:\n",
    "- 无监督训练模式：skipgram或者cbow，默认是skipgram,在实践中,skipgram模式在利用子词方面比cbow更好\n",
    "- 词嵌入维度dim:默认为100，但随着语料库的增大，词嵌入的维度往往也要更大\n",
    "- 学习率lr:默认是0.05,根据经验，建议选择[0.01, 1]范围内\n",
    "- 使用线程数thread：默认是12个线程，一般建议和CPU核心数相同"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('data/fil9', \"cbow\", dim=300, epoch=1, lr=0.1, thread=8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 第四步：模型效果检验\n",
    "- 检查单词向量质量的一种简单的方法就是查看其邻近单词，通过我们主观判断这些单词是否与目标单词相关程度来粗略评定模型效果好坏"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#查找”运动“的邻近单词，我们可以发现”体育网“.\"运动汽车\"，”运动服“等\n",
    "model.get_nearest_neighbors(\"sports\")\n",
    "# 查找\"音乐\"的邻近单词，我们可以发现音乐有关的词汇\n",
    "model.get_nearest_neighbors(\"music\")\n",
    "\n",
    "# 查找”小狗“的邻近单词，我们可以发现小狗有关的词汇\n",
    "model.get_nearest_neighbors('dog')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 第五步:模型的保存与重加载"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 使用save_model保存模型\n",
    "model.save_model('fil9.bin')\n",
    "\n",
    "# 使用fasttext.laod_model加载模型\n",
    "model = fasttext.load_model('fil9.bin')\n",
    "model.get_word_vector(\"the\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 什么是word embedding(词嵌入)\n",
    "- 通过一定的方式将词汇映射到指定维度(一般是更高维度)的空间\n",
    "- 广义的word embedding包括所有密集词汇向量的表示方法，如之前学习的word2vec,即可认为是word embedding的一种\n",
    "- 狭义的word embedding是指神经网络中加入的embedding层，对整个网络进行训练的同时产生embedding矩阵(embedding层的参数)，这个embedding矩阵就是训练过程中所有输入词汇的向量表示组成的矩阵"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入torch和tensorboard导入进来\n",
    "import torch\n",
    "import fileinput\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 实例化一个写入对象\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# 随机初始化一个100*5的矩阵, 将其视作已经得到的词嵌入矩阵\n",
    "embedded = torch.randn(100, 50)\n",
    "\n",
    "# 导入事先准备好的100个中文词汇文件, 形成meta列表原始词汇\n",
    "meta = list(map(lambda x: x.strip(), fileinput.FileInput(\"./vocab100.csv\")))\n",
    "writer.add_embedding(embedded, metadata=meta)\n",
    "writer.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 在终端启动tensorboard服务\n",
    "- tensorboard --logdir runs --host 0.0.0.0\n",
    "- 通过http://0.0.0.0:6006访问浏览器可视化页面"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 小结\n",
    "\n",
    "##### 学习什么是文本张量表示\n",
    "\n",
    "- 将一段文本使用张量进行表示，其中一般将词汇表示成向量:称作词向量，再由各个词向量按顺序组成矩阵形成文本表示\n",
    "\n",
    "##### 学习文本张量表示的作用\n",
    "\n",
    "-  将文本表示成张量(矩阵)形式，能够使语言文本可以作为计算机处理程序的输入，进行接下来的一系列的解析工作\n",
    "\n",
    "##### 学习文本张量表示的方法\n",
    "\n",
    "- one-hot编码\n",
    "- Word2vec\n",
    "- Word Embedding\n",
    "\n",
    "##### 学习了什么是one-hot词向量表示\n",
    "\n",
    "-  又称热独编码，将每个词表示成具有n个元素的向量，这个词向量中只有一个元素是1，其他元素都是0，不同词汇元素为0的位置不同，其中n的大小是整个语料中不同词汇的总数\n",
    "- one-hot编码的优劣势\n",
    "\t- 优势：操作简单，容易理解\n",
    "\t- 劣势：完全割裂了词与词之间的联系，而且在语料集下，每个向量和长度过长，占据大量内存\n",
    "\n",
    "#### 学习了什么是word2vec\n",
    "\n",
    "- 是一种流行的将词汇表示成向量的无监督训练方法，该过程将构建神经网络模型，将网络参数作为词汇的向量表示，它包含CBOW和skipgram两种训练模式\n",
    "- CBOW(Continuous bag of words)模式\n",
    "\t- 给定一段用于训练的文本语料，再选定某段长度(窗口)作为研究对象，使用上下文词汇预测目标词汇\n",
    "\t- 假设我们给定的训练语料只有一句话:Hope can set you free(愿你自由成长)，窗口大小为3，因此模型的第一个训练样本来自Hope you set,因为是CBOW模式，所以将使用Hope和set作为输入，you作为输出，在模型训练时，Hope,set,you等词汇都使用它们的one-hot编码，如图所示:每个one-hot编码的单词与各自的变换矩阵(即参数矩阵3x5,这里的3是指最后得到的词向量维度)相乘之后再相加，得到上下文表示矩阵(3x1)\n",
    "\t- 接着，将上下文表示矩阵与变换矩阵(参数矩阵5x3，所的有变换矩阵共享参数)相乘，得到5x1的结果矩阵，它将与我们真正的目标矩阵即you的one-hot编码矩阵(5x1)进行损失的计算，然后更新网络参数完成一次模型迭代\n",
    "\t- 最后窗口按序向后移动，重新更新参数，直到所有的语料被遍历完成 ，得到最终的变换矩阵(3x5),这个变换矩阵与每个词汇的one-hot编码(5x1)相乘，得到3x1的矩阵就是该词汇的word2vec张量表示\n",
    "- skipgram模式\n",
    "\t- 给定一段用于训练的文本语料，再选定某段长度(窗口)作为研究对象，使用目标词汇预测上下文词汇\n",
    "\t-  假设我们给定的训练语料只有一句话:Hope can set you free(愿你自由成长)，窗口大小为3，因此模型的第一个训练样本来自Hope you set，因为是skipgram模式，所以将使用you作为输入，hope和set作为输出,在模型训练时，Hope，set，you等词汇都使用它们的one-hot编码，将you的one-hot编码与变换矩阵(即参数矩阵3x5,这里的3是指最后得到的词向量维度)相乘，得到目标词汇表示矩阵(3x1)\n",
    "\t- 接着，将目标词汇表示矩阵与多个变换矩阵(参数矩阵5x3)相乘，得到5x1的结果矩阵，它将与我们hope和set对应的one-hot编码矩阵(5x1)进行损失的计算，然后更新网络参数完成一次模型迭代\n",
    "\t- 最后窗口按序向后移动，重新更新参数，直到所有语料被遍历完成，得到最终的变换矩阵即参数矩阵(3x5).这个变换矩阵与每个词汇的one-hot编码(5x1)相乘，得到的3x1的矩阵就是该词汇的word2vec张量表示\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
