{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### 新闻主题分类任务\n",
    "\n",
    "- 以一段新闻报道中的文本描述内容为输入，使用模型帮助我们判断它最有可能属于哪一种类型的新闻，这是典型的文本分类问题，我们这里假定每种类型是互斥的，即文本描述有且只有一种类型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入相关的torch工具包\n",
    "import torch\n",
    "import torchtext\n",
    "# 导入torchtext.datasets import text_classification\n",
    "import os\n",
    "\n",
    "# 定义数据下载路径，当前路径的data文件夹\n",
    "load_data_path = \"./data\"\n",
    "# 如果不存在该路径，则创建这个路径\n",
    "if not os.path.isdir(load_data_path):\n",
    "    os.mkdir(load_data_path)\n",
    "\n",
    "# 选取torchtext中的文本分类数据集\"AG_NEWS\"即新闻主题分类数据，保存在指定目录下\n",
    "# 并将数据映射后的训练和验证数据加载到内存中\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](root=load_data_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 整个案例的实现可分为以下五个步骤\n",
    "- 第一步:构建带有Embedding层的文本分类模型\n",
    "- 第二步:对数据进行batch处理\n",
    "- 第三步:构建训练与验证函数\n",
    "- 第四步:进行模型训练和验证\n",
    "- 第五步:查看embedding层嵌入的词向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 第一步: 构建带有Embedding层的文本分类模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入必备的torch模型的构建工具\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 指定BATCH_SIZE的大小\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# 进行可用设备检测，有GPU的话将优先使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    \"\"\"文本分类模型\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        \"\"\"\n",
    "        description: 类的初始化函数\n",
    "        :param vocab_size: 整个语料包含的不同词汇总数\n",
    "        :param embed_dim: 指定词嵌入的维度\n",
    "        :param num_classes: 文本分类的类别总数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 实例化embedding层，sparse=True代表每次该层求解梯度时，只更新部分权重\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, sparse=True)\n",
    "        # 实例化线性层，参数分别是embed_dim和num_class\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        # 为各层初始化权重\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化权重函数\"\"\"\n",
    "        # 指定初始权重的取值范围数\n",
    "        initrange = 0.5\n",
    "        # 各层的权重参数都是初始化为均匀分布\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        # 偏置初始化为0\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        :param text: 文本数值映射后的结果\n",
    "        :return: 与类别尺寸相同的数量，用以判断文本类别\n",
    "        \"\"\"\n",
    "        # 获得embedding的结果embedded\n",
    "        # >>> embedded.shape\n",
    "        # (m, 32)其中m是BATCH_SIZE大小的数据中语汇总数\n",
    "        embedded = self.embedding(text)\n",
    "        # 接下来我们需要将(m, 32)转化成(BATCH_SIZE, 32)\n",
    "        # 以便通过fc层后能计算相应的损失\n",
    "        # 首先，我们已知m的值大于BATCH_SIZE =  16\n",
    "        # 用m整除BATCH_SIZE，获得m中共包含c个BATCH_SIZE\n",
    "        c = embedded.size(0) // BATCH_SIZE\n",
    "        # 之后再从embedding中取c*BATCH_SIZE个向量得到新的embedded\n",
    "        # 这个新的embedded中的向量个数可以整除BATCH_SIZE\n",
    "        embedded = embedded[:BATCH_SIZE*c]\n",
    "        # 因为我们想利用平均池化的方法求embeded中指定行数的列的平均数\n",
    "        # 但平均池化方法是作用在行上的，并且需要3维输入\n",
    "        # 因此我们对新的embedded进行转置并拓展维度\n",
    "        embedded = embedded.transpose(1, 0).unsqueeze(0)\n",
    "        # 然后就是调用平均池化的方法，并且核的大小为c\n",
    "        # 即取每c的元素计算一次均值作为结果\n",
    "        embedded = F.avg_pool1d(embedded, kernel_size = c)\n",
    "        # 最后，还需要减去新增的维度，然后转置回去输送给fc层\n",
    "        return self.fc(embedded[0].transpose(1,0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 实例化模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获得整个语料包含不同词汇总数\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "# 指定词嵌入维度\n",
    "EMBED_DIM = 32\n",
    "# 获得类别总数\n",
    "NUM_CLASS = len(train_dataset.get_labels())\n",
    "# 实例化模型\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第二步：对数据进行batch处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    \"\"\"\n",
    "    description: 生成batch数据函数\n",
    "    :param batch: 由样本张量和对应标签的无组组成的batch_size大小的列表\n",
    "                  形如:\n",
    "                  [(label1, sample1), (label2, sample2),..., (labelN, sampleN)]\n",
    "    :return: 样本张量和标签各自的列表形式(张量)\n",
    "             形如:\n",
    "            text = tensor([sample1, sample2,..., sampleN])\n",
    "            label = tensor([label1, label2,..., labelN])\n",
    "    \"\"\"\n",
    "    # 从batch中获得标签张量\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    # 从batch中获取样本张量\n",
    "    text = [entry[1] for entry in batch]\n",
    "    text = torch.cat(text)\n",
    "    # 返回结果\n",
    "    return text, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 调用"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 假设一个输入:\n",
    "batch = [(1, torch.tensor([3, 23, 2, 8])), (0, torch.tensor([3, 45, 21, 6]))]\n",
    "res = generate_batch(batch)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建训练和验证函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入torch中的数据加载器方法\n",
    "from  torch.utils.data import DataLoader\n",
    "\n",
    "# 定义损失函数, 定义交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 定义优化器, 定义随机梯度下降优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "# 定义优化器步长的一个优化器, 专门用于学习率的衰减\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "\n",
    "def train(train_data):\n",
    "    \"\"\"模型训练函数\"\"\"\n",
    "    # 初始化训练损失和准确率为0\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    # 使用数据加载器生成BATCH_SIZE大小的数据进行批次训练\n",
    "    # data就是N多个generate_batch函数处理后的BATCH_SIZE大小的数据生成器\n",
    "    data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "    for i, (text, cls) in enumerate(data):\n",
    "        # 设置优化器的初始梯度为0\n",
    "        optimizer.zero_grad()\n",
    "        # 模型输入一个批次数据，获得输出\n",
    "        output = model(text)\n",
    "        # 根据真实标签与模型输出计算损失\n",
    "        loss = criterion(output, cls)\n",
    "        # 将该批次的损失加到总损失中\n",
    "        train_loss += loss.item()\n",
    "        # 误差反向传播\n",
    "        loss.backward()\n",
    "        # 参数进行更新\n",
    "        optimizer.step()\n",
    "        # 将该批次的准确率加到总准确率中\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "\n",
    "    # 调整优化器学习率\n",
    "    scheduler.step()\n",
    "\n",
    "    # 返回本轮训练的平均损失和平均准确率\n",
    "    return train_loss / len(train_data), train_acc / len(train_data)\n",
    "\n",
    "\n",
    "def valid(valid_data):\n",
    "    \"\"\"模型验证函数\"\"\"\n",
    "    # 初始化验证损失和准确率为0\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    # 和训练相同，使用DataLoader获得训练数据生成器\n",
    "    data = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    # 按批次取出数据进行验证\n",
    "    for text, cls in data:\n",
    "        # 验证阶段，不再求解梯度\n",
    "        with torch.no_grad():\n",
    "            # 使用模型获得输出\n",
    "            output = model(text)\n",
    "            # 计算损失\n",
    "            loss = criterion(output, cls)\n",
    "            # 将损失和准确率加到总损失的准确率中\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "    # 返回本轮验证的平均损失和平均准确率\n",
    "    return  loss / len(valid_data), acc / len(valid_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第四步:进行模型训练和验证"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "# 导入数据随机划分方法工具\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# 指定训练轮数\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# 定义初始化的验证损失\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "\n",
    "# 从train_dataset取出0.95作为训练集, 先取其长度\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "\n",
    "sub_train_, sub_valid_ = random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "# 开始每一轮训练\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # 记录开始训练的时间\n",
    "    start_time = time.time()\n",
    "    # 调用train和valid函数得到训练和验证的平均损失，平均准确率\n",
    "    train_loss, train_acc = train(sub_train_)\n",
    "    valid_loss, valid_acc = valid(sub_valid_)\n",
    "\n",
    "    # 计算训练和验证的总耗时(秒)\n",
    "    secs = int(time.time() - start_time)\n",
    "    # 用分钟和秒表示\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    # 打印训练和验证耗时，平均损失，平均准确率\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## |第五步: 查看embedding层嵌入的词向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 打印从模型的状态字典中获得的Embedding矩阵\n",
    "print(model.state_dict()['embedding.weight'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
