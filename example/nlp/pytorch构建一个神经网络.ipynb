{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### 关于torch.nn\n",
    "- 使用pytorch来构建神经网络，主要工具都在torch.nn包中\n",
    "- nn依赖于autograd来定义模型，并对其自动求导\n",
    "\n",
    "#### 构建神经网络的典型流程\n",
    "\n",
    "- 定义一个拥有可学习参数的神经网络\n",
    "- 遍历训练数据集\n",
    "- 处理输入数据使其流经神经网络\n",
    "- 计算损失值\n",
    "- 将网络参数的梯度进行反向传播\n",
    "- 以一定的规则更新网络的权重"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 导入苦于工具包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义一个简单的网络类\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 定义第一层的卷积神经网络，输入通道维度=1，输出通道维度=6.卷积核大小3*3\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        # 定义第二层卷积神经网络，输入通道维度=6,输出通道维度=16，卷积核大小3*3\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # 定义第三层全连接网络\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在(2, 2)的池化窗口下执行最大池化操作\n",
    "        # 注意任意卷积层后面要加激活层，池化层\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # 经过卷积层处理后，张量要进入全连接层，进入前需要调整张量的形状\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        # 计算size, 除了第0个维度上的batch_size\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:36:42.864373Z",
     "end_time": "2023-06-07T10:36:44.377653Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 模型中所有的可训练参数，可以通过net.parameters()来获得"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[[ 0.1816,  0.0147,  0.2244],\n",
      "          [ 0.1292,  0.1517, -0.0884],\n",
      "          [ 0.1117, -0.2432,  0.2255]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1592, -0.2725, -0.1455],\n",
      "          [-0.2495, -0.2141,  0.2920],\n",
      "          [ 0.0224, -0.0857, -0.1947]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1007,  0.2706, -0.2832],\n",
      "          [ 0.1719,  0.0083,  0.3183],\n",
      "          [-0.2251,  0.0013,  0.0948]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1859,  0.0681,  0.1555],\n",
      "          [-0.1378,  0.0475, -0.1863],\n",
      "          [-0.1889,  0.2218,  0.0465]]],\n",
      "\n",
      "\n",
      "        [[[-0.1656, -0.0866, -0.0454],\n",
      "          [ 0.0077, -0.1432,  0.3081],\n",
      "          [-0.0744,  0.0683, -0.2291]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1331, -0.2354, -0.1769],\n",
      "          [-0.2991, -0.0809, -0.2159],\n",
      "          [-0.1794, -0.0724,  0.0489]]]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3195,  0.2838, -0.2865,  0.3039, -0.0223,  0.0338],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.1089,  0.0135,  0.0281],\n",
      "          [-0.0429,  0.0312,  0.1337],\n",
      "          [-0.0328, -0.0523, -0.0020]],\n",
      "\n",
      "         [[-0.0066, -0.1133, -0.1021],\n",
      "          [-0.0696, -0.0625, -0.0729],\n",
      "          [ 0.0065, -0.0627, -0.0490]],\n",
      "\n",
      "         [[-0.0611, -0.0715, -0.1205],\n",
      "          [ 0.1153,  0.1195, -0.0403],\n",
      "          [-0.0874,  0.1153, -0.1164]],\n",
      "\n",
      "         [[-0.1006, -0.0207,  0.0563],\n",
      "          [ 0.0450, -0.1201, -0.0112],\n",
      "          [-0.1210, -0.1304, -0.0920]],\n",
      "\n",
      "         [[ 0.0436,  0.1300, -0.0329],\n",
      "          [ 0.0271, -0.0519, -0.0199],\n",
      "          [ 0.0662,  0.0138, -0.1057]],\n",
      "\n",
      "         [[ 0.0174,  0.0770,  0.0168],\n",
      "          [ 0.0732,  0.0648, -0.1092],\n",
      "          [-0.1264,  0.0342, -0.0298]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0397,  0.0454, -0.1114],\n",
      "          [ 0.0652,  0.0193, -0.0572],\n",
      "          [-0.0018, -0.0247,  0.0840]],\n",
      "\n",
      "         [[-0.0469, -0.1010,  0.0748],\n",
      "          [-0.0221, -0.0723, -0.0315],\n",
      "          [ 0.0527,  0.0603, -0.0935]],\n",
      "\n",
      "         [[ 0.0407,  0.1142, -0.0705],\n",
      "          [-0.0619, -0.1145,  0.0335],\n",
      "          [-0.0290, -0.0906, -0.1050]],\n",
      "\n",
      "         [[-0.1202,  0.1276, -0.0824],\n",
      "          [-0.0338, -0.0058,  0.0569],\n",
      "          [-0.1232, -0.0143, -0.0953]],\n",
      "\n",
      "         [[ 0.1224,  0.0981, -0.0341],\n",
      "          [ 0.0596, -0.0119,  0.1108],\n",
      "          [ 0.1043,  0.0313, -0.0843]],\n",
      "\n",
      "         [[ 0.0706,  0.0205,  0.0130],\n",
      "          [-0.0808,  0.0638, -0.0553],\n",
      "          [ 0.1333, -0.1182, -0.0184]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0494,  0.0543, -0.0108],\n",
      "          [-0.1194,  0.1241, -0.0357],\n",
      "          [-0.0936, -0.0175, -0.1144]],\n",
      "\n",
      "         [[-0.0572,  0.1257, -0.0729],\n",
      "          [-0.0184, -0.1093,  0.0231],\n",
      "          [-0.1196, -0.1266,  0.0936]],\n",
      "\n",
      "         [[-0.0654,  0.0990,  0.1307],\n",
      "          [ 0.1316,  0.1321,  0.0006],\n",
      "          [-0.0940,  0.0535,  0.1199]],\n",
      "\n",
      "         [[-0.0311, -0.0184, -0.1019],\n",
      "          [-0.0088,  0.0879,  0.0805],\n",
      "          [-0.1272,  0.0582,  0.0639]],\n",
      "\n",
      "         [[ 0.0764, -0.0977,  0.1038],\n",
      "          [ 0.0417,  0.0069,  0.1018],\n",
      "          [ 0.0847, -0.1337,  0.0330]],\n",
      "\n",
      "         [[-0.0073,  0.0313,  0.0446],\n",
      "          [-0.1216, -0.0828,  0.0643],\n",
      "          [-0.1011, -0.0555,  0.0626]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0191, -0.0835, -0.1314],\n",
      "          [ 0.0206,  0.0114,  0.0076],\n",
      "          [ 0.0547, -0.0568, -0.0543]],\n",
      "\n",
      "         [[-0.0604, -0.0049,  0.0603],\n",
      "          [ 0.0195, -0.0787,  0.1309],\n",
      "          [ 0.0106,  0.1341, -0.1094]],\n",
      "\n",
      "         [[-0.1101,  0.0503,  0.0403],\n",
      "          [-0.0948,  0.0320,  0.0385],\n",
      "          [-0.0247, -0.0656,  0.0390]],\n",
      "\n",
      "         [[-0.0434, -0.0505, -0.1122],\n",
      "          [ 0.1160,  0.0632,  0.0715],\n",
      "          [ 0.0583,  0.1147, -0.0027]],\n",
      "\n",
      "         [[ 0.0066,  0.0521,  0.0362],\n",
      "          [ 0.1110,  0.0342, -0.0816],\n",
      "          [-0.0774, -0.0250,  0.0303]],\n",
      "\n",
      "         [[ 0.0155, -0.0674,  0.0192],\n",
      "          [-0.0211, -0.1082,  0.0853],\n",
      "          [ 0.0883, -0.0907, -0.0912]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1154, -0.0234,  0.1357],\n",
      "          [-0.0888,  0.0394,  0.1091],\n",
      "          [-0.0259,  0.0223,  0.0721]],\n",
      "\n",
      "         [[-0.0275, -0.0065,  0.0223],\n",
      "          [-0.0317,  0.1080, -0.0425],\n",
      "          [ 0.0484,  0.1052,  0.0947]],\n",
      "\n",
      "         [[ 0.0861, -0.1191, -0.1328],\n",
      "          [-0.0634,  0.0406, -0.0336],\n",
      "          [-0.0214,  0.0080, -0.0497]],\n",
      "\n",
      "         [[ 0.0702,  0.0509,  0.0986],\n",
      "          [ 0.0389, -0.0518, -0.1182],\n",
      "          [ 0.1150, -0.0460,  0.1337]],\n",
      "\n",
      "         [[ 0.1247, -0.1325,  0.0726],\n",
      "          [ 0.0848,  0.1089,  0.1353],\n",
      "          [-0.0260, -0.0731,  0.1025]],\n",
      "\n",
      "         [[ 0.1248,  0.0327, -0.1288],\n",
      "          [-0.1071, -0.0533,  0.0584],\n",
      "          [ 0.0784,  0.0879, -0.1070]]],\n",
      "\n",
      "\n",
      "        [[[-0.0595, -0.0746,  0.0758],\n",
      "          [ 0.0072, -0.1330,  0.0163],\n",
      "          [-0.0844, -0.0748,  0.0047]],\n",
      "\n",
      "         [[-0.0039,  0.0672, -0.0065],\n",
      "          [ 0.1228,  0.1055, -0.0659],\n",
      "          [-0.0015, -0.0023,  0.0409]],\n",
      "\n",
      "         [[ 0.1029, -0.0725,  0.0672],\n",
      "          [ 0.0753,  0.1276,  0.0658],\n",
      "          [ 0.0286,  0.0343,  0.0689]],\n",
      "\n",
      "         [[ 0.1141,  0.1010,  0.0454],\n",
      "          [ 0.0272,  0.0988, -0.0466],\n",
      "          [ 0.0568,  0.0743, -0.0354]],\n",
      "\n",
      "         [[ 0.0740,  0.1209, -0.0398],\n",
      "          [-0.0495, -0.0496,  0.1116],\n",
      "          [ 0.0016, -0.0960, -0.1344]],\n",
      "\n",
      "         [[ 0.0916, -0.1330,  0.0748],\n",
      "          [ 0.1087,  0.0257, -0.1136],\n",
      "          [ 0.0925,  0.0818,  0.0307]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0278,  0.0131, -0.0917],\n",
      "          [ 0.1130, -0.0612, -0.0117],\n",
      "          [-0.0251,  0.0786,  0.0702]],\n",
      "\n",
      "         [[-0.0305,  0.0470, -0.1062],\n",
      "          [ 0.1235,  0.0959,  0.0599],\n",
      "          [-0.1071, -0.0358, -0.0765]],\n",
      "\n",
      "         [[ 0.1122, -0.0906,  0.0638],\n",
      "          [-0.0162, -0.0847,  0.0705],\n",
      "          [ 0.1205, -0.0804, -0.0619]],\n",
      "\n",
      "         [[ 0.0778,  0.0801,  0.1234],\n",
      "          [ 0.1303,  0.0168,  0.0667],\n",
      "          [ 0.0728, -0.0464, -0.0409]],\n",
      "\n",
      "         [[ 0.1284, -0.0241, -0.0199],\n",
      "          [ 0.0875,  0.0107,  0.0751],\n",
      "          [-0.0448, -0.0096,  0.0601]],\n",
      "\n",
      "         [[-0.0091,  0.1265, -0.0977],\n",
      "          [-0.1059, -0.0102,  0.0765],\n",
      "          [ 0.0474,  0.0440, -0.0041]]],\n",
      "\n",
      "\n",
      "        [[[-0.0435, -0.0841, -0.1038],\n",
      "          [ 0.0689,  0.1089, -0.1107],\n",
      "          [ 0.0158,  0.0858,  0.1184]],\n",
      "\n",
      "         [[ 0.0054,  0.1335, -0.1340],\n",
      "          [-0.1102,  0.0588,  0.1346],\n",
      "          [-0.0459, -0.0592,  0.0934]],\n",
      "\n",
      "         [[ 0.0680, -0.0138,  0.0955],\n",
      "          [ 0.1202,  0.1361,  0.0213],\n",
      "          [-0.0533, -0.0334,  0.0627]],\n",
      "\n",
      "         [[-0.0941, -0.0435, -0.1039],\n",
      "          [-0.0339, -0.0948,  0.0765],\n",
      "          [ 0.1076, -0.1307, -0.0354]],\n",
      "\n",
      "         [[ 0.0408, -0.0422,  0.1062],\n",
      "          [ 0.0420,  0.0133,  0.1234],\n",
      "          [ 0.0170,  0.0269,  0.1024]],\n",
      "\n",
      "         [[-0.1243,  0.0966, -0.0636],\n",
      "          [ 0.0510, -0.0552,  0.0900],\n",
      "          [ 0.0027, -0.0420, -0.0594]]],\n",
      "\n",
      "\n",
      "        [[[-0.1332, -0.1202, -0.0705],\n",
      "          [ 0.0860,  0.0146, -0.0801],\n",
      "          [ 0.1142, -0.0963,  0.0537]],\n",
      "\n",
      "         [[-0.1080, -0.0453, -0.1133],\n",
      "          [-0.0604, -0.0655,  0.1025],\n",
      "          [-0.0552,  0.1179,  0.0061]],\n",
      "\n",
      "         [[ 0.1017, -0.0521,  0.0487],\n",
      "          [ 0.0135,  0.0418, -0.0428],\n",
      "          [ 0.0569,  0.1160, -0.0237]],\n",
      "\n",
      "         [[-0.0501,  0.1338, -0.0464],\n",
      "          [ 0.0332,  0.0486, -0.0144],\n",
      "          [ 0.1257, -0.0140,  0.1245]],\n",
      "\n",
      "         [[-0.0904, -0.0721, -0.1031],\n",
      "          [ 0.0755, -0.0898, -0.0122],\n",
      "          [-0.0585, -0.1157,  0.1231]],\n",
      "\n",
      "         [[ 0.0003, -0.1203, -0.1063],\n",
      "          [-0.0271, -0.0515,  0.1205],\n",
      "          [ 0.0830, -0.0300,  0.0357]]],\n",
      "\n",
      "\n",
      "        [[[-0.0140,  0.1170,  0.0081],\n",
      "          [ 0.0659,  0.1224, -0.1355],\n",
      "          [ 0.0547, -0.1102,  0.0079]],\n",
      "\n",
      "         [[ 0.0574, -0.1126, -0.0144],\n",
      "          [ 0.0202, -0.0598,  0.0049],\n",
      "          [ 0.0603, -0.0497, -0.0901]],\n",
      "\n",
      "         [[-0.0754,  0.0873,  0.0075],\n",
      "          [ 0.1138, -0.0022,  0.0449],\n",
      "          [-0.1209, -0.1316, -0.1168]],\n",
      "\n",
      "         [[ 0.0590,  0.0865,  0.0374],\n",
      "          [ 0.0533, -0.0982,  0.0918],\n",
      "          [ 0.1279,  0.0964,  0.1205]],\n",
      "\n",
      "         [[-0.1002, -0.0295, -0.1016],\n",
      "          [ 0.0687,  0.0154,  0.0521],\n",
      "          [-0.1222, -0.0709, -0.1043]],\n",
      "\n",
      "         [[-0.0118, -0.0344,  0.0783],\n",
      "          [ 0.1011, -0.0080,  0.0487],\n",
      "          [-0.0628, -0.0178,  0.0599]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0950, -0.0064, -0.1188],\n",
      "          [ 0.0863,  0.1028, -0.0632],\n",
      "          [-0.1059, -0.1066, -0.1221]],\n",
      "\n",
      "         [[-0.1223,  0.0491,  0.1022],\n",
      "          [ 0.0730,  0.0440, -0.0712],\n",
      "          [-0.1310,  0.0503, -0.1125]],\n",
      "\n",
      "         [[-0.0080,  0.0976, -0.0093],\n",
      "          [ 0.0490,  0.1207, -0.0906],\n",
      "          [-0.1334, -0.0912,  0.0306]],\n",
      "\n",
      "         [[ 0.0009,  0.1317,  0.0582],\n",
      "          [-0.0278,  0.0888,  0.1125],\n",
      "          [-0.1100, -0.0476,  0.1214]],\n",
      "\n",
      "         [[ 0.0200, -0.0054,  0.1224],\n",
      "          [-0.0339,  0.1332,  0.0161],\n",
      "          [-0.0725,  0.0732,  0.0880]],\n",
      "\n",
      "         [[-0.0654,  0.1342,  0.0812],\n",
      "          [ 0.0802, -0.0464, -0.0350],\n",
      "          [ 0.1070,  0.0248,  0.0319]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1176, -0.0475, -0.0023],\n",
      "          [ 0.0922,  0.0727, -0.0479],\n",
      "          [-0.0816, -0.1023,  0.1119]],\n",
      "\n",
      "         [[ 0.1025, -0.1296,  0.0575],\n",
      "          [-0.0557, -0.0497, -0.0822],\n",
      "          [-0.0212, -0.0248,  0.0314]],\n",
      "\n",
      "         [[-0.0430,  0.0644, -0.0925],\n",
      "          [ 0.0980, -0.0350,  0.0223],\n",
      "          [-0.0975, -0.1026,  0.1173]],\n",
      "\n",
      "         [[ 0.0491,  0.1111,  0.1151],\n",
      "          [-0.0409,  0.0907, -0.0864],\n",
      "          [-0.0815, -0.0521, -0.1271]],\n",
      "\n",
      "         [[ 0.0370, -0.0287, -0.1230],\n",
      "          [ 0.0419,  0.1246,  0.0536],\n",
      "          [ 0.0853,  0.0435, -0.0388]],\n",
      "\n",
      "         [[-0.0480,  0.0560,  0.1319],\n",
      "          [ 0.0686,  0.1057,  0.1212],\n",
      "          [-0.0377, -0.1185, -0.1125]]],\n",
      "\n",
      "\n",
      "        [[[-0.1239, -0.0103, -0.0596],\n",
      "          [-0.0213,  0.1139,  0.0478],\n",
      "          [-0.0077,  0.1057,  0.0817]],\n",
      "\n",
      "         [[ 0.1238,  0.0540,  0.1025],\n",
      "          [-0.0278,  0.1007,  0.0671],\n",
      "          [ 0.0797, -0.0192,  0.1292]],\n",
      "\n",
      "         [[-0.0172,  0.0125,  0.1192],\n",
      "          [-0.0727,  0.0562,  0.0629],\n",
      "          [ 0.1300,  0.0961, -0.0232]],\n",
      "\n",
      "         [[-0.0909, -0.1193, -0.1188],\n",
      "          [-0.1295, -0.0321,  0.1359],\n",
      "          [ 0.0061,  0.0585, -0.0531]],\n",
      "\n",
      "         [[ 0.0666, -0.0827,  0.0451],\n",
      "          [ 0.0261,  0.1295, -0.1110],\n",
      "          [ 0.0751,  0.0007, -0.1039]],\n",
      "\n",
      "         [[-0.1231,  0.0452,  0.1306],\n",
      "          [-0.0662, -0.1345, -0.0255],\n",
      "          [ 0.1109,  0.0672,  0.1153]]],\n",
      "\n",
      "\n",
      "        [[[-0.0310,  0.1020,  0.0113],\n",
      "          [-0.1020,  0.1230,  0.0582],\n",
      "          [ 0.0809, -0.0699, -0.0365]],\n",
      "\n",
      "         [[ 0.0944,  0.1134,  0.1201],\n",
      "          [-0.1133,  0.0369,  0.0149],\n",
      "          [ 0.0314, -0.0411,  0.1226]],\n",
      "\n",
      "         [[ 0.0813, -0.1260,  0.0369],\n",
      "          [ 0.0150, -0.0605, -0.0297],\n",
      "          [ 0.0245, -0.0816, -0.0191]],\n",
      "\n",
      "         [[ 0.1228, -0.1174,  0.1258],\n",
      "          [-0.1130,  0.0045, -0.0770],\n",
      "          [-0.0378,  0.0414,  0.1046]],\n",
      "\n",
      "         [[-0.1220, -0.0513, -0.0422],\n",
      "          [-0.0298, -0.1072,  0.1246],\n",
      "          [-0.0685,  0.0091, -0.0353]],\n",
      "\n",
      "         [[-0.0757, -0.0832,  0.0208],\n",
      "          [ 0.0598,  0.0950,  0.1233],\n",
      "          [-0.0124,  0.0142,  0.0399]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1277, -0.0384, -0.1033],\n",
      "          [-0.0741,  0.0537, -0.0388],\n",
      "          [ 0.1270,  0.1070, -0.1247]],\n",
      "\n",
      "         [[ 0.1144,  0.0408, -0.1145],\n",
      "          [-0.0487,  0.0532, -0.0978],\n",
      "          [-0.0042,  0.1329, -0.1203]],\n",
      "\n",
      "         [[ 0.0063,  0.0576,  0.0082],\n",
      "          [ 0.0519,  0.0226,  0.0077],\n",
      "          [ 0.1314, -0.0176,  0.1056]],\n",
      "\n",
      "         [[ 0.0602,  0.0820, -0.0306],\n",
      "          [-0.0938, -0.1017, -0.0781],\n",
      "          [-0.0975,  0.1153, -0.1164]],\n",
      "\n",
      "         [[-0.1123, -0.0472,  0.0821],\n",
      "          [-0.0754, -0.1038, -0.1054],\n",
      "          [-0.0903,  0.0570,  0.0480]],\n",
      "\n",
      "         [[ 0.1331, -0.0616,  0.0871],\n",
      "          [-0.0187,  0.0986, -0.0422],\n",
      "          [-0.0503, -0.1076, -0.1030]]],\n",
      "\n",
      "\n",
      "        [[[-0.0290, -0.0129, -0.0590],\n",
      "          [ 0.0159, -0.1079, -0.0194],\n",
      "          [ 0.0586,  0.1127, -0.0714]],\n",
      "\n",
      "         [[ 0.1051,  0.0785,  0.0227],\n",
      "          [ 0.1180, -0.0989,  0.1270],\n",
      "          [-0.0545,  0.0766,  0.0504]],\n",
      "\n",
      "         [[-0.0843,  0.1016,  0.0122],\n",
      "          [ 0.0041, -0.1020, -0.1259],\n",
      "          [-0.1280,  0.1316, -0.0637]],\n",
      "\n",
      "         [[-0.0011,  0.0118,  0.0647],\n",
      "          [-0.1113,  0.0547,  0.1309],\n",
      "          [-0.1032,  0.0555,  0.0927]],\n",
      "\n",
      "         [[-0.1015, -0.0034,  0.0560],\n",
      "          [-0.0527, -0.0818, -0.0840],\n",
      "          [ 0.0390, -0.0102,  0.1078]],\n",
      "\n",
      "         [[ 0.1024,  0.0351,  0.1226],\n",
      "          [ 0.1120, -0.1223,  0.0371],\n",
      "          [ 0.0593, -0.0190,  0.1232]]]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0373, -0.1352,  0.1341,  0.1133, -0.0104,  0.0832,  0.0640, -0.0038,\n",
      "        -0.1103,  0.0728,  0.0167, -0.0224, -0.1191, -0.0682,  0.0928,  0.0939],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0138, -0.0157,  0.0184,  ..., -0.0206,  0.0008,  0.0129],\n",
      "        [-0.0230,  0.0398,  0.0150,  ..., -0.0221,  0.0217,  0.0315],\n",
      "        [-0.0005, -0.0264, -0.0313,  ..., -0.0262, -0.0146,  0.0336],\n",
      "        ...,\n",
      "        [ 0.0376,  0.0275,  0.0227,  ..., -0.0143, -0.0273, -0.0095],\n",
      "        [ 0.0001, -0.0248, -0.0165,  ...,  0.0260, -0.0229, -0.0404],\n",
      "        [ 0.0391, -0.0098, -0.0391,  ..., -0.0399, -0.0412,  0.0250]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0166, -0.0330,  0.0402,  0.0073, -0.0385,  0.0173, -0.0389, -0.0226,\n",
      "        -0.0133, -0.0098,  0.0386,  0.0073, -0.0346, -0.0259, -0.0346,  0.0356,\n",
      "        -0.0267,  0.0063, -0.0068, -0.0255, -0.0199,  0.0317, -0.0079, -0.0158,\n",
      "        -0.0126,  0.0324,  0.0264, -0.0286, -0.0379,  0.0008,  0.0398,  0.0171,\n",
      "        -0.0007, -0.0394, -0.0198,  0.0347, -0.0195, -0.0126,  0.0008, -0.0379,\n",
      "        -0.0191, -0.0328, -0.0271, -0.0018,  0.0259,  0.0208, -0.0147, -0.0170,\n",
      "         0.0344, -0.0343,  0.0180, -0.0104, -0.0298,  0.0151, -0.0119,  0.0392,\n",
      "        -0.0288,  0.0092,  0.0410, -0.0114, -0.0065,  0.0239, -0.0044,  0.0295,\n",
      "         0.0305, -0.0326, -0.0334, -0.0301,  0.0141, -0.0049,  0.0398, -0.0185,\n",
      "         0.0092, -0.0230,  0.0006,  0.0366, -0.0116, -0.0039,  0.0193, -0.0175,\n",
      "        -0.0342, -0.0236, -0.0007,  0.0131,  0.0277, -0.0175, -0.0093,  0.0021,\n",
      "        -0.0085,  0.0280,  0.0229, -0.0202, -0.0217, -0.0293, -0.0276, -0.0214,\n",
      "         0.0221,  0.0379, -0.0016, -0.0405, -0.0091, -0.0078, -0.0307,  0.0380,\n",
      "        -0.0360,  0.0070, -0.0204,  0.0325, -0.0206, -0.0273,  0.0112,  0.0106,\n",
      "         0.0065,  0.0053,  0.0328, -0.0330,  0.0226, -0.0214,  0.0187, -0.0140],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0212, -0.0186, -0.0072,  ...,  0.0567,  0.0191,  0.0712],\n",
      "        [ 0.0154,  0.0323, -0.0813,  ...,  0.0056,  0.0143,  0.0275],\n",
      "        [ 0.0379,  0.0578, -0.0413,  ...,  0.0151,  0.0138,  0.0032],\n",
      "        ...,\n",
      "        [-0.0360,  0.0089, -0.0592,  ...,  0.0511, -0.0251, -0.0563],\n",
      "        [-0.0051,  0.0483,  0.0576,  ...,  0.0346, -0.0158, -0.0780],\n",
      "        [-0.0464,  0.0157, -0.0273,  ...,  0.0078, -0.0592, -0.0308]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 5.4603e-02, -7.4539e-02, -1.6340e-02, -6.7455e-02, -1.1198e-02,\n",
      "        -8.5367e-03, -7.1420e-02, -7.6511e-02,  6.7704e-02, -3.2295e-02,\n",
      "        -2.1416e-02, -8.7148e-02, -1.5795e-02,  7.9669e-02,  8.3782e-02,\n",
      "         1.1683e-02, -5.5774e-02, -5.1974e-02, -6.5533e-05, -2.0837e-02,\n",
      "        -7.1073e-02,  7.8861e-02, -7.4663e-02, -4.8268e-02,  4.0682e-02,\n",
      "        -1.2509e-02,  3.1165e-02,  6.0013e-02, -8.7734e-02,  1.8082e-02,\n",
      "         8.7284e-02, -3.2762e-02, -8.6543e-02,  6.5935e-02,  4.8332e-02,\n",
      "        -8.2576e-02,  7.2635e-02,  5.2688e-02,  2.3032e-02,  2.7039e-02,\n",
      "        -6.9453e-02, -1.6211e-02,  7.2154e-03,  1.9589e-02,  5.8798e-02,\n",
      "        -7.0958e-02, -2.1685e-02, -4.8973e-02, -6.0847e-03, -8.4340e-02,\n",
      "         2.9000e-02, -5.7111e-02,  8.8351e-03, -2.2594e-02, -3.1587e-03,\n",
      "        -2.2792e-02, -3.9830e-02, -8.8312e-02,  8.1421e-02,  4.3641e-02,\n",
      "        -2.4532e-02, -2.0280e-02, -7.6457e-02,  4.1308e-02,  3.2883e-02,\n",
      "        -7.0250e-02,  7.7346e-02, -9.0008e-02, -9.0942e-02, -6.0238e-03,\n",
      "        -2.2017e-02,  2.4693e-02, -5.3774e-02,  4.7524e-02, -9.1212e-02,\n",
      "         7.0578e-02,  9.1740e-03,  8.7578e-02, -1.2396e-02,  7.9119e-02,\n",
      "         6.5664e-02, -1.6671e-02, -9.0655e-02, -4.0251e-02],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-6.7531e-02, -5.8769e-02, -5.4610e-02,  8.3833e-03, -9.6117e-02,\n",
      "          5.3022e-02,  1.1368e-02, -1.0156e-01, -7.4105e-02, -5.7335e-03,\n",
      "          7.2392e-03,  1.4969e-02, -9.2723e-02, -9.3942e-02,  8.0916e-02,\n",
      "          1.0155e-01, -5.2916e-02, -1.1826e-03,  9.0375e-02,  7.1889e-02,\n",
      "         -3.2871e-02,  9.7401e-02,  6.4344e-02,  3.0639e-03, -6.5737e-03,\n",
      "         -7.8396e-02,  6.6469e-02, -1.4149e-03,  9.1919e-02,  1.0694e-01,\n",
      "         -2.5716e-02, -8.7930e-02, -1.7281e-02, -7.8908e-02,  7.0122e-02,\n",
      "          1.4977e-02, -9.5361e-03, -2.5025e-02,  6.6749e-02, -8.3192e-02,\n",
      "         -6.4085e-02,  9.3680e-02,  2.6376e-02, -1.2715e-02,  1.0326e-01,\n",
      "         -7.6929e-02,  5.6299e-02,  1.0752e-01, -4.7883e-02,  4.5955e-02,\n",
      "          1.5605e-03,  5.9544e-02, -1.8866e-02,  5.9315e-02, -8.4761e-03,\n",
      "          2.7674e-02, -7.3787e-02, -2.5871e-03,  7.6505e-02,  5.0232e-02,\n",
      "         -1.7088e-02, -6.1430e-03, -5.5322e-02,  9.7840e-02, -1.3373e-02,\n",
      "         -1.0336e-01,  1.0406e-01, -2.0011e-02,  8.3786e-02, -1.9738e-02,\n",
      "         -3.4872e-02,  3.0745e-02,  2.4784e-03, -1.0770e-01, -1.0577e-01,\n",
      "         -9.7534e-03, -5.6662e-02, -5.7987e-02,  2.2912e-03, -9.5942e-02,\n",
      "         -1.0235e-02, -1.8236e-02,  3.9635e-02, -3.6731e-02],\n",
      "        [-6.3558e-02, -2.6235e-03,  5.1579e-02, -4.2549e-02,  1.0438e-01,\n",
      "          9.2314e-02, -4.9153e-02,  8.1912e-02,  7.3269e-02, -1.0003e-01,\n",
      "         -6.2986e-02, -7.3150e-02,  3.6413e-02, -6.9232e-02,  2.9545e-02,\n",
      "          5.2553e-02,  8.6920e-03,  6.0280e-02, -6.8462e-02,  8.8180e-02,\n",
      "          2.5759e-02, -8.0937e-02, -3.6229e-02,  3.1590e-02,  1.0563e-01,\n",
      "          7.4217e-02,  5.2785e-02, -8.0309e-02,  1.0905e-01,  7.4995e-02,\n",
      "         -9.2675e-02, -9.4688e-02, -1.4710e-02, -3.6233e-02,  5.2034e-02,\n",
      "          1.0497e-01,  9.3463e-02, -3.6301e-02, -2.8496e-03,  9.0647e-03,\n",
      "          5.3566e-02, -1.5284e-02,  1.2645e-02, -1.6271e-02,  7.6971e-02,\n",
      "          2.5234e-02, -5.1594e-02, -4.6328e-02, -5.7936e-02,  8.6262e-03,\n",
      "          4.1772e-02, -5.3714e-02,  8.1613e-02,  6.0155e-02,  4.2854e-02,\n",
      "         -7.1237e-02, -9.8813e-02, -1.9023e-02, -2.3459e-02, -1.0084e-01,\n",
      "         -9.7467e-02,  7.0915e-02, -3.2459e-02,  7.3536e-02, -1.0904e-01,\n",
      "         -1.0065e-01,  1.0711e-01, -8.7798e-02, -3.7965e-02,  1.4141e-02,\n",
      "          3.9557e-02,  9.4194e-02, -9.5321e-02, -4.2397e-02, -5.4917e-02,\n",
      "          1.0618e-01, -1.3650e-02, -4.2146e-02,  4.9206e-02,  7.8259e-03,\n",
      "         -1.0357e-01,  9.1777e-02,  2.9259e-02,  5.5050e-02],\n",
      "        [-6.4368e-02,  1.4064e-02,  1.0715e-01, -1.4635e-03,  4.9424e-02,\n",
      "          8.4917e-02,  7.7633e-02,  3.1153e-02,  6.3048e-02, -6.9806e-02,\n",
      "         -5.7728e-02,  8.6897e-02, -5.7375e-03,  1.7936e-02, -1.6422e-02,\n",
      "          7.7197e-02, -1.0767e-01, -5.3237e-02,  1.0579e-01, -1.0508e-01,\n",
      "          8.5154e-02, -9.9426e-02, -7.8033e-02,  6.1437e-02,  8.9658e-02,\n",
      "          2.8579e-02,  9.8720e-02,  3.1240e-02, -6.9262e-02, -3.5315e-02,\n",
      "         -3.9485e-02,  6.1864e-02,  2.4325e-02,  5.7707e-02, -9.6580e-02,\n",
      "          5.4759e-02, -8.6898e-02, -8.0767e-02,  4.2748e-02, -1.0157e-01,\n",
      "          8.2795e-02,  6.8206e-02,  2.4621e-02,  3.0194e-02, -9.2347e-03,\n",
      "         -1.1158e-03, -8.6299e-02,  8.5984e-02, -1.9183e-02,  1.8057e-02,\n",
      "          4.2149e-02, -6.8640e-02,  4.3904e-02,  6.0861e-02, -4.1273e-02,\n",
      "         -3.0542e-03, -8.4888e-02,  1.0344e-01,  9.9625e-03, -5.2929e-02,\n",
      "          2.0786e-02, -7.7982e-03,  6.2256e-02,  8.5595e-02,  8.5801e-02,\n",
      "         -2.7673e-02,  1.0649e-01, -3.5028e-02, -3.9580e-02,  8.4886e-03,\n",
      "          8.0591e-02, -4.7184e-02,  2.6543e-02,  9.2784e-02, -9.1658e-02,\n",
      "          2.4854e-02,  1.6730e-02,  3.6980e-02, -5.7122e-02,  3.3938e-02,\n",
      "          8.9708e-02,  3.3696e-02, -7.7883e-02,  4.9444e-02],\n",
      "        [-6.9183e-02, -1.0257e-02,  8.6143e-02,  1.0621e-01, -1.1738e-02,\n",
      "         -7.4379e-02, -1.3566e-02, -2.7881e-02, -8.9141e-02,  4.5895e-02,\n",
      "          1.7962e-02, -2.1806e-02, -4.4125e-02,  1.0502e-01,  1.1916e-03,\n",
      "         -1.0176e-01,  1.5969e-02, -7.7648e-02, -2.5618e-02,  4.1731e-02,\n",
      "         -9.5750e-02, -2.8053e-02,  5.2226e-02,  6.7505e-02, -4.7333e-04,\n",
      "         -9.2261e-02,  9.0429e-02,  7.2515e-03,  6.8845e-02,  6.9171e-02,\n",
      "         -9.3035e-02,  7.5672e-02, -5.5295e-02, -9.6420e-02, -5.9504e-03,\n",
      "         -6.1112e-02,  4.7668e-02, -5.3036e-02,  2.3103e-02, -8.5634e-02,\n",
      "         -1.0205e-01, -8.8503e-02, -2.5194e-02, -5.7241e-02, -8.0554e-02,\n",
      "          1.0317e-01,  2.5572e-02, -9.2105e-02, -2.1368e-02, -5.6241e-02,\n",
      "          6.1585e-02,  6.5039e-02,  1.0047e-01,  6.7518e-02,  1.0552e-01,\n",
      "          6.2589e-02,  5.6311e-02,  3.6001e-02, -1.0371e-02,  1.0658e-01,\n",
      "          1.0304e-01, -1.0200e-01,  5.7133e-02,  6.4927e-02, -3.3810e-02,\n",
      "          4.7162e-03, -8.2188e-02, -1.8174e-02,  9.2001e-02,  2.2722e-02,\n",
      "         -6.6710e-02, -7.0588e-02,  2.2406e-02, -1.0109e-02,  1.0322e-01,\n",
      "          1.9665e-02,  4.1342e-02, -7.0697e-02, -1.0916e-02, -8.2117e-02,\n",
      "          4.8763e-03, -8.0029e-02, -1.7532e-02, -1.3180e-02],\n",
      "        [-4.8554e-02,  2.5018e-02,  6.2321e-02, -9.8933e-02, -1.0029e-02,\n",
      "          9.5487e-02, -5.8930e-03,  1.6564e-02,  1.0607e-01,  5.7998e-02,\n",
      "          7.6240e-02, -6.2082e-02,  3.7378e-02, -4.8269e-03, -5.7823e-02,\n",
      "         -3.7354e-02, -4.8715e-02, -2.7260e-02, -1.0414e-01,  9.0992e-02,\n",
      "         -8.0960e-02, -1.0771e-01,  3.5634e-02,  8.9351e-03,  3.8830e-03,\n",
      "         -1.0881e-03,  6.7202e-02,  8.7191e-02, -7.0303e-02,  4.0324e-02,\n",
      "         -8.4919e-03,  1.0099e-01, -6.1950e-03, -3.3770e-02, -2.0978e-02,\n",
      "          4.6919e-02,  6.1326e-02, -4.1327e-02,  9.1711e-02, -9.4113e-02,\n",
      "         -4.4361e-02, -7.3237e-02,  7.1807e-02,  7.7372e-02,  3.3359e-02,\n",
      "          9.1756e-02, -6.8892e-04,  7.6472e-02,  3.7841e-02, -3.5425e-03,\n",
      "          9.6165e-02,  9.2895e-02,  6.9858e-02,  9.9565e-02, -1.3927e-02,\n",
      "         -1.3080e-02,  2.6788e-03,  1.0658e-01,  2.7535e-02,  8.6967e-02,\n",
      "          1.8536e-02,  4.6805e-02,  2.8216e-02,  1.7978e-03, -1.3731e-02,\n",
      "          3.2234e-02,  1.3454e-02,  6.6046e-02,  5.8566e-02, -6.5937e-02,\n",
      "         -4.8206e-02, -4.8448e-02, -3.1369e-02, -4.2789e-02, -3.5738e-02,\n",
      "         -4.8127e-03,  5.6851e-02, -6.3206e-02,  9.4824e-02, -9.0215e-02,\n",
      "         -7.4747e-02, -7.9640e-02, -8.3560e-03, -4.8193e-02],\n",
      "        [ 3.1858e-02, -5.7012e-02,  7.9821e-02, -6.3739e-03, -9.0278e-02,\n",
      "          1.4180e-02, -3.0822e-02, -2.5002e-03,  1.0223e-01, -4.7126e-03,\n",
      "          4.4914e-02,  4.5143e-03,  1.0164e-01,  1.5745e-02,  8.4643e-02,\n",
      "         -4.1704e-02,  2.6940e-03,  7.0426e-02, -6.7707e-02,  3.0166e-02,\n",
      "         -3.6577e-02, -1.0062e-01,  3.0828e-02, -3.4332e-03,  6.2728e-02,\n",
      "          9.6634e-02,  6.0964e-02, -5.3774e-02,  5.3633e-02, -1.7829e-02,\n",
      "         -3.2557e-02, -8.9476e-02, -2.0820e-02, -2.5841e-02, -9.1305e-02,\n",
      "         -1.0861e-01, -4.7402e-02, -7.2550e-02,  7.6528e-02, -2.6751e-02,\n",
      "         -1.7149e-02,  7.3310e-02, -6.0315e-03, -5.1600e-02,  6.8857e-02,\n",
      "         -1.0711e-01, -2.2871e-02, -2.8831e-02,  6.4349e-02, -8.0206e-02,\n",
      "         -8.3791e-02, -2.9579e-02, -2.6671e-03,  3.3810e-02,  7.0855e-02,\n",
      "         -1.8132e-02,  8.8429e-02,  6.0938e-02,  7.1550e-02,  2.0997e-02,\n",
      "          1.0015e-01, -1.6018e-02,  6.4740e-02,  4.0700e-03, -8.9265e-02,\n",
      "          4.0252e-02, -3.1882e-02, -1.6087e-03, -9.9548e-02,  2.1859e-02,\n",
      "         -8.4069e-02, -1.8527e-02,  1.0030e-01,  1.9737e-04, -4.9806e-02,\n",
      "          4.7469e-02, -2.9933e-02, -1.5191e-02, -6.7946e-02,  1.9610e-02,\n",
      "         -1.0821e-01, -6.1026e-02, -8.2748e-02, -1.5664e-02],\n",
      "        [-8.8651e-03,  1.4057e-02, -8.2473e-02,  4.1833e-03,  4.3768e-02,\n",
      "         -4.6664e-02,  3.2110e-03, -8.4768e-02, -8.2143e-02,  1.0009e-01,\n",
      "          8.2361e-02,  5.9803e-02,  1.0451e-01, -7.5311e-02,  3.0850e-02,\n",
      "         -2.3957e-02, -6.7125e-02,  6.0627e-02, -8.4590e-02,  6.0067e-03,\n",
      "          8.6355e-02,  3.9592e-02, -7.4764e-02,  5.3198e-02, -2.8144e-02,\n",
      "          8.9951e-02,  4.9840e-02, -3.7567e-02, -3.3288e-02,  5.3914e-02,\n",
      "         -1.0581e-01,  8.5149e-02,  5.7545e-03, -6.0560e-02,  9.3131e-02,\n",
      "          9.7586e-02, -7.3593e-02, -9.0653e-02,  2.7068e-02,  9.2216e-02,\n",
      "          4.5169e-02, -1.2473e-02,  3.6184e-02, -5.2613e-02, -8.0841e-02,\n",
      "         -4.1213e-02,  6.9374e-02,  7.6650e-02, -6.3534e-02, -2.7584e-03,\n",
      "          1.4811e-02,  1.0528e-01, -5.2509e-02,  9.5651e-02, -5.6109e-02,\n",
      "         -2.4012e-03, -7.8791e-02,  4.1473e-02, -9.8213e-02,  8.4459e-02,\n",
      "          4.0131e-03,  8.9788e-02,  2.8413e-02,  6.5435e-02, -1.4217e-02,\n",
      "         -3.8329e-02,  2.1583e-02,  7.0197e-02,  1.1658e-02,  7.5819e-02,\n",
      "          7.0593e-02,  3.5697e-02,  1.0242e-01, -1.7557e-02, -8.0445e-02,\n",
      "          6.0078e-02,  1.4201e-02,  2.4253e-02, -9.6719e-03, -5.3643e-03,\n",
      "         -5.7460e-02,  8.1347e-02,  2.9797e-02, -1.0189e-01],\n",
      "        [ 6.5698e-02,  1.0266e-01,  7.2755e-02, -2.9578e-02, -1.1515e-02,\n",
      "          9.5525e-02,  2.1155e-02, -4.5776e-02,  9.3013e-02, -6.4796e-02,\n",
      "         -9.2091e-02,  8.3142e-02, -6.8102e-02,  8.9653e-02, -2.7282e-02,\n",
      "          3.2993e-02,  3.7271e-02,  3.2150e-02,  8.9100e-03, -4.4095e-02,\n",
      "          6.1677e-02, -5.9118e-02,  5.3353e-02,  8.4359e-02, -2.1575e-02,\n",
      "         -4.7854e-02,  6.5216e-03,  6.4602e-02, -4.8249e-02,  8.1624e-02,\n",
      "          6.5019e-02,  9.8893e-02,  3.0134e-02,  3.6935e-02,  6.9544e-02,\n",
      "         -2.0695e-03, -3.1948e-03,  4.5709e-02,  7.1804e-02, -5.3411e-02,\n",
      "         -9.7574e-02, -9.3196e-02,  4.7930e-02,  8.3680e-03, -9.1135e-02,\n",
      "         -7.1884e-02,  2.3657e-03, -9.1078e-02,  2.2724e-02, -5.4868e-02,\n",
      "         -1.0983e-02, -4.6983e-02, -8.0391e-02, -1.7586e-02, -2.6845e-02,\n",
      "         -7.0242e-02,  1.1603e-02,  7.5894e-02, -9.3662e-02, -2.5827e-02,\n",
      "         -7.0853e-03,  8.2924e-02,  3.9727e-02, -3.2501e-02, -5.5759e-02,\n",
      "         -1.0076e-01,  4.2855e-02, -2.2746e-02, -9.2047e-02, -8.8207e-02,\n",
      "          5.5258e-02, -9.7892e-02, -9.2492e-02,  5.0293e-02, -1.0308e-01,\n",
      "         -4.5395e-02, -5.6040e-02, -1.1967e-02,  9.6244e-02,  4.6431e-02,\n",
      "         -3.9878e-02,  1.8065e-02, -6.8584e-02, -1.0359e-01],\n",
      "        [ 5.2005e-02,  7.5561e-02, -2.1109e-02, -9.8128e-02, -5.0862e-02,\n",
      "          3.6931e-02, -3.2529e-02, -3.0727e-02,  2.3672e-02,  7.8074e-02,\n",
      "          3.6173e-02,  6.1686e-02, -8.1961e-02,  3.2265e-02, -8.5429e-03,\n",
      "          2.0411e-02, -9.1205e-02,  4.5087e-02, -8.8448e-02,  3.7025e-02,\n",
      "         -5.9685e-02, -5.3028e-02,  5.9157e-02,  1.0827e-01,  8.5053e-02,\n",
      "          7.9252e-02, -6.9315e-02,  3.8765e-03, -6.4745e-02, -9.7607e-02,\n",
      "          3.2049e-02, -1.7253e-02, -1.0036e-01, -2.6163e-02, -8.5537e-02,\n",
      "         -7.7004e-02, -9.0577e-02,  6.7301e-02, -1.7488e-02, -8.4869e-02,\n",
      "          2.9978e-02,  4.0807e-02, -8.0747e-02, -1.0864e-01, -3.5954e-02,\n",
      "         -1.9073e-02,  1.8560e-02, -4.2407e-02,  5.9856e-02, -6.0562e-02,\n",
      "          9.5297e-02, -5.1177e-03, -3.1624e-02, -4.4539e-02, -4.8583e-02,\n",
      "         -8.0428e-02, -3.4105e-02, -2.7536e-02, -1.0878e-01,  4.5708e-02,\n",
      "         -4.3106e-02,  8.7454e-02, -2.8854e-02,  4.4491e-02,  6.7027e-02,\n",
      "          5.7828e-02, -6.9631e-02, -1.0336e-01, -2.5241e-02,  1.0080e-01,\n",
      "         -9.9027e-02,  3.6224e-02,  8.5658e-02, -8.7669e-02, -8.7173e-02,\n",
      "         -1.8748e-03,  3.2963e-02,  9.4466e-02, -5.6874e-02, -3.8942e-02,\n",
      "          7.8045e-02, -1.0309e-01, -6.8421e-02, -1.0013e-02],\n",
      "        [ 6.3286e-02, -6.9211e-02, -4.2481e-02,  5.3829e-03, -9.8811e-02,\n",
      "         -1.0106e-01, -4.6088e-02, -1.3306e-02, -7.1115e-02, -8.1727e-03,\n",
      "          2.9060e-02,  5.2405e-02, -9.3931e-02, -1.8014e-02,  7.7562e-02,\n",
      "          8.4241e-02, -6.4900e-02, -8.1334e-02,  8.7287e-04,  7.8163e-02,\n",
      "         -7.6162e-02,  3.7325e-02, -6.3643e-02, -3.0983e-02,  8.7252e-03,\n",
      "         -8.3857e-02, -9.1445e-02, -3.2538e-02,  1.0474e-01,  4.2897e-02,\n",
      "         -5.4145e-02,  4.4918e-03,  4.9016e-04,  9.5471e-02,  4.6680e-03,\n",
      "         -3.1966e-02, -9.3261e-02,  8.5526e-02,  4.9940e-02,  7.9262e-02,\n",
      "         -1.4201e-02, -6.9787e-02, -2.4069e-02,  7.7907e-02, -7.4619e-03,\n",
      "          1.0391e-01, -3.4617e-02,  5.9806e-02, -1.1792e-03, -3.5763e-02,\n",
      "         -5.7336e-02, -1.0374e-01,  9.4844e-02, -6.0389e-02, -6.6938e-02,\n",
      "          3.6853e-03,  5.4028e-02, -9.6278e-02,  3.5699e-02, -8.1881e-02,\n",
      "         -8.7689e-02,  2.4082e-02, -6.1285e-02, -4.0921e-02,  6.7514e-02,\n",
      "         -4.1968e-02,  1.8763e-02,  9.2714e-02, -2.0069e-05,  8.1029e-02,\n",
      "          7.3215e-02,  2.0232e-02,  1.0447e-03, -3.9365e-02, -8.5555e-02,\n",
      "         -1.8043e-04, -7.8250e-02,  9.4022e-02,  9.4747e-02,  3.6400e-02,\n",
      "          6.4384e-02,  7.0907e-02, -9.6391e-02, -5.7317e-02]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0809,  0.1053, -0.0137, -0.1030,  0.0193,  0.0359,  0.0753,  0.0575,\n",
      "         0.0453,  0.0028], requires_grad=True)]\n",
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(params)\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:42:51.890829Z",
     "end_time": "2023-06-07T10:42:51.927461Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 假设图像的输入尺寸为32*32"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1022,  0.1310,  0.0664, -0.0728,  0.0986,  0.0410,  0.0835,  0.0882,\n",
      "          0.0722, -0.0710]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:43:17.386819Z",
     "end_time": "2023-06-07T10:43:17.427532Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 有了输出张量后，就可以执行梯度归零和反向传播的操作了"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:48:25.846597Z",
     "end_time": "2023-06-07T10:48:25.888752Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 注意\n",
    "- torch.nn构建的神经网络只支持mini-batches的输入，不支持单一样本的输入\n",
    "- 比如: nn.Conv2d需要一个4D的Tensor，形状为(nSamples, nChannels, Height, Width)如果你的输入只有单一样本形式，则只需要执行input.unsqueeze(0).主动将3D Tensor扩充成4D Tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 损失函数\n",
    "- 损失函数的输入是一个输入的pair(output, target)然后计算出一个数值来评估output和target之间的差距大小\n",
    "- 在torch.nn中在若干不同的损失函数可供使用，比如nn.MSELoss就是通过计算均方差损失来评估输入和目标值之间的差距\n",
    "\n",
    "> 下面是一个MSTLoss计算损失的例子"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1022,  0.1310,  0.0664, -0.0728,  0.0986,  0.0410,  0.0835,  0.0882,\n",
      "          0.0722, -0.0710]], grad_fn=<AddmmBackward0>)\n",
      "tensor([-1.6489,  0.2414, -2.5625, -0.2139,  0.2378,  1.3533, -1.4623, -0.0681,\n",
      "         0.7402,  0.2568])\n",
      "tensor([[-1.6489,  0.2414, -2.5625, -0.2139,  0.2378,  1.3533, -1.4623, -0.0681,\n",
      "          0.7402,  0.2568]])\n",
      "tensor(1.4045, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "print(output)\n",
    "target = torch.randn(10)\n",
    "print(target)\n",
    "# 改变target形状为二维张量，为了和output匹配\n",
    "target = target.view(1, -1)\n",
    "print(target)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:34:22.725233Z",
     "end_time": "2023-06-07T11:34:22.774305Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 关于方向传播的链条，如果我们跟踪loss反向传播方向，使用.grad_fn属性打印，将可以看到一张完整的计算图如下\n",
    "- Input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss\n",
    "\n",
    "- 当调用loss.backward()时，整张计算图将对loss进行自动求导，所有属性requires_grad=True的Tensors都将参与梯度求导的运算，并将梯度累加到Tensors中的.grad属性中"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x7fcc4ef7f820>\n",
      "<AddmmBackward0 object at 0x7fcc4ef7ee30>\n",
      "<AccumulateGrad object at 0x7fcc4ef7f820>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:40:41.007553Z",
     "end_time": "2023-06-07T11:40:41.028137Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 反向传播(backpropagation)\n",
    "- 在pytorch中执行反向传播非常方便，全部的操作就是loss.backward()\n",
    "- 在执行反向传播前，要先将梯度清零，否则梯度会不同的批次数据之间被累加"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0123,  0.0403, -0.0084,  0.0084, -0.0032,  0.0161])\n"
     ]
    }
   ],
   "source": [
    "# pytorch中执行梯度清零的代码\n",
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "# pytorch中执行反向传播的代码\n",
    "loss.backward()\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T14:23:16.888199Z",
     "end_time": "2023-06-07T14:23:16.921691Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 更新网络参数\n",
    "- 更新参数最简单的算法就是SGD(随机梯度下降)\n",
    "- 具体的算法公式表达式为:weight = weight - learning_rate * gradient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 首先用传统的python代码来实现SGD如下:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T14:29:12.102743Z",
     "end_time": "2023-06-07T14:29:12.137873Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用pytorch官方推荐的标准代码如下:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 首先导入优化器的包，optim中包含若干常用的优化算法，比如SGD，Adam等\n",
    "import torch.optim as optim\n",
    "\n",
    "# 通过optim创建优化器对象\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# 将优化器执行梯度清零操作\n",
    "optimizer.zero_grad()\n",
    "\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# 对损失值执行反向传播的操作\n",
    "loss.backward()\n",
    "# 参数的更新通过一行标准代码来执行\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T14:49:25.163516Z",
     "end_time": "2023-06-07T14:49:25.175971Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 小节总结\n",
    "\n",
    "- 学习了构建一个神经网络的典型流程\n",
    "\t-  定义一个拥有可学习参数的神经网络\n",
    "\t- 遍历训练数据集\n",
    "\t- 处理输入数据使其流经神经网络\n",
    "\t- 计算损失值\n",
    "\t- 将网络参数的梯度进行反向传播\n",
    "\t- 以一定的规则更新网络权重\n",
    "- 学习了损失函数的定义\n",
    "\t- 采用torch.nn.MSELoss()计算均方误差\n",
    "\t- 通过loss.backward()进行反向传播计算时，整张计算图将对loss进行自动求导，所有属性requires_grad=True的Tensors都将参与梯度求导的运算，并将梯度累加到Tensors中的,grad属性中\n",
    "- 学习了反向传播的计算方法\n",
    "\t- 在pytorch中执行反向传播非常简便，全部的操作就是loss.backward()\n",
    "\t- 在执行反向传播之前，要先将梯度清零，否则梯度会在不同的批次数据之间被累加\n",
    "\t\t- net.zero_grad()\n",
    "\t\t- loss.backward()\n",
    "- 学习了参数更新方法\n",
    "\t- 定义优化器来执行参数的优化与更新\n",
    "\t\t- optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\t- 通过优化器来执行具体的参数更新\n",
    "\t\t- optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
