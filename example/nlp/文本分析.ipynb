{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<center> <h1>文本数据分析</h1> </center>\n",
    "\n",
    "#### 学习目标\n",
    "- 了解文本数据分析的作用\n",
    "- 掌握常用的几种文本数据分析方法\n",
    "\n",
    "#### 文本数据分析的作用\n",
    "- 文本数据分析能够有效帮助我们理解数据语料，快速检查出语料可能存在的问题，并指导之后模型训练过程中一些超参数的选择\n",
    "\n",
    "#### 常用的几种文本数据分析方法\n",
    "- 标签数量分布\n",
    "- 句子长度分布\n",
    "- 词频统计与关键词词云"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入必备的工具包\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "\n",
    "# 设置显示风格\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# 利用pandas读取训练数据和验证数据\n",
    "train_data = pd.read_csv(\"./cn_data/train.tsv\", sep=\"\\t\")\n",
    "valid_data = pd.read_csv(\"./cn_data/dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 获得训练数据标签的数量分布\n",
    "sns.countplot(\"label\", data=train_data)\n",
    "plt.title(\"train_data\")\n",
    "plt.show()\n",
    "\n",
    "# 获得验证数据标签的数量分布\n",
    "sns.countplot(\"label\", data=valid_data)\n",
    "plt.title(\"valid_data\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 分析\n",
    "\n",
    "- 在深度学习模型评估中，我们一般使用ACC作为评估指标，若想将ACC的基线定义在50%左右，则需要我们的正负样本比例维持在1:1左右，否则就要进行必要的数据增加或数据删减，上面图中训练和验证集正负样本都稍有不均衡，可以进行一些数据增强"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 获取训练集和验证集的句子长度分布"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 在训练数据中添加新的句子长度列，每个元素的值都是对应的句子列的长度\n",
    "train_data[\"sentence_length\"] = list(map(lambda x:len(x), train_data[\"sentence\"]))\n",
    "\n",
    "# 绘制句子长度列的数量分布图\n",
    "sns.countplot(\"sentence_length\", data=train_data)\n",
    "# 主要关注count长度分布的纵坐标，不需要绘制横坐标，横坐标范围通过dist图进行查看\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "\n",
    "# 绘制dist长度分布图\n",
    "sns.displot(train_data[\"sentence_length\"])\n",
    "\n",
    "# 主要关注dist长度分布横坐标，不需要绘制纵坐标\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# 在验证数据中添加新的句子长度列，每个元素的值都是对应的句子列的长度\n",
    "valid_data[\"sentence_length\"] = list(map(lambda x :len(x), valid_data[\"sentence\"]))\n",
    "\n",
    "# 绘制句子长度列的数量分布图\n",
    "sns.countplot(\"sentence_length\", data=valid_data)\n",
    "\n",
    "# 主要关注count长度分布的纵坐标，不需要绘制横坐标，横坐标范围通过dist图进行查看\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "\n",
    "# 绘制dist长度分布图\n",
    "sns.displot(valid_data[\"sentence_length\"])\n",
    "\n",
    "# 主要关注count长度分布的横坐标，不需要绘制纵坐标\n",
    "plt.xticks([])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 分析\n",
    "- 通过绘制句子长度分布图，可以得知我们的语料中大部分句子长度的分布范围，因为模型的输入要求为固定尺寸的张量，合理的长度范围对之后进行句子的截断补齐(规范长度)起到关键的指导作用，上图中大部分句子长度的范围大致在20-250之间"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 获取训练集和验证集的正负样本长度散点分布"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 绘制训练集长度分布的散点图\n",
    "sns.stripplot(y='sentence_length', x='label', data=train_data)\n",
    "plt.show()\n",
    "\n",
    "# 绘制验证集长度分布的散点图\n",
    "sns.stripplot(y='sentence_length', x='label', data=valid_data)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 分析\n",
    "- 通过查看正负样本长度散点图，可以有效定位异常点的出现位置，帮助我们更准确进行人式语料审查，上图中在训练集正样本中出现了异常点，它的句子长度近3500左右，需要我们人工审查"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 获得训练集与验证集不同词汇总数的统计"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入jieba用于分词\n",
    "# 导入chain方法用于扁平化列表\n",
    "import jieba\n",
    "from  itertools import chain\n",
    "\n",
    "# 进行训练集的句子进行分词，并统计出不同词汇的总数\n",
    "train_vocab = set(chain(*map(lambda x: jieba.lcut(x), train_data['sentence'])))\n",
    "print(\"训练集共包含不同词汇总数为:\", len(train_vocab))\n",
    "\n",
    "# 进行验证集的句子进行分词，并统计出不同词汇的总数\n",
    "valid_vocab = set(chain(*map(lambda x: jieba.lcut(x), valid_data['sentence'])))\n",
    "print(\"训练集共包含不同词汇总数为:\", len(valid_vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 获得训练集上正负样本的高频形容词词云"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 使用jieba中的词性标注功能\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "def get_a_list(text):\n",
    "    \"\"\"用于获取形容词列表\"\"\"\n",
    "    # 使用jieba的词性标注方法切分文本，获得具有词性属性flag和词汇属性word的对象\n",
    "    # 从而判断flag是否为形容词，来返回对应的词汇\n",
    "    r = []\n",
    "    for g in pseg.lcut(text):\n",
    "        if g.flag == \"a\":\n",
    "            r.append(g.word)\n",
    "    return r\n",
    "\n",
    "# 导入绘制词云的工具包\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def get_word_cloud(keywords_list):\n",
    "    # 实例化绘制词云的类，其中参数font_path是字体路径，为了能够显示中文\n",
    "    # max_words指词云图像最多显示多少个词，background_color为背影颜色\n",
    "    wordcloud = WordCloud(font_path=\"./SimHei.ttf\", max_words=100, background_color=\"white\")\n",
    "    # 将传入的列表转化成词云生成器需要的字符串形式\n",
    "    keywords_string = \" \".join(keywords_list)\n",
    "    # 生成词云\n",
    "    wordcloud.generate(keywords_string)\n",
    "\n",
    "    # 绘制图像并显示\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 获得训练集上正样本\n",
    "p_train_data = train_data[train_data[\"label\"] == 1][\"sentence\"]\n",
    "\n",
    "# 获得正样本的每个句子的形容词\n",
    "train_p_a_vocab = chain(*map(lambda x :get_a_list(x), p_train_data))\n",
    "# print(train_p_a_vocab)\n",
    "\n",
    "# 获得训练集上负样本\n",
    "n_train_data = train_data[train_data[\"label\"] == 0][\"sentence\"]\n",
    "\n",
    "# 获得负样本的每个句子的形容词\n",
    "train_n_a_vocab = chain(*map(lambda x :get_a_list(x), n_train_data))\n",
    "\n",
    "# 调用绘制词云函数\n",
    "get_word_cloud(train_p_a_vocab)\n",
    "get_word_cloud(train_n_a_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获得验证集上正样本\n",
    "p_valid_data = valid_data[valid_data[\"label\"] == 1][\"sentence\"]\n",
    "\n",
    "# 获得正样本的每个句子的形容词\n",
    "valid_p_a_vocab = chain(*map(lambda x :get_a_list(x), p_valid_data))\n",
    "# print(valid_p_a_vocab)\n",
    "\n",
    "# 获得验证集上负样本\n",
    "n_valid_data = valid_data[valid_data[\"label\"] == 0][\"sentence\"]\n",
    "\n",
    "# 获得负样本的每个句子的形容词\n",
    "valid_n_a_vocab = chain(*map(lambda x :get_a_list(x), n_valid_data))\n",
    "\n",
    "# 调用绘制词云函数\n",
    "get_word_cloud(valid_p_a_vocab)\n",
    "get_word_cloud(valid_n_a_vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 分析\n",
    "- 根据高频形容词词云显示，我们可以对当前语料质量进行简单评估，同时对违反语料标签含义的词汇进行人工审查和修正，来保证绝大多数的语料符合训练标准，上图中的正样本大多数是褒义词，而负样本大多数是贬义词，基本符合要求，但是负样本词云中也存在\"便利\"这样的褒义词，因此可以进行人工审查"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 小结\n",
    "- 学习文本数据分析的作用\n",
    "\t- 文本数据分析能够有效帮助我们理解数据语料，快速检查语料可能存在的问题，并指导之后模型训练过程中一些超参数的选择\n",
    "\n",
    "- 学习常用的几种文本数据分析方法\n",
    "\t- 标签数量分布\n",
    "\t- 句子长度分布\n",
    "\t- 词频统计与关键词词云\n",
    "- 学习基于真实的中文酒店评论语料进行几种文本数据分析方法\n",
    "\t- 获得训练集和验证集标签数量分布\n",
    "\t- 获得训练集和验证集的句子长度分布\n",
    "\t- 获得训练集和验证集正负样本长度散点分布\n",
    "\t- 获得训练集和验证集不同词汇总数统计\n",
    "\t- 获得训练集和验证集正负样本的高频形容词词云"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 导入必备的工具包\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "\n",
    "# 设置显示风格\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# 利用pandas读取训练数据和验证数据\n",
    "train_data = pd.read_csv(\"./cn_data/train.tsv\", sep=\"\\t\")\n",
    "valid_data = pd.read_csv(\"./cn_data/dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 获得训练数据标签的数量分布\n",
    "# sns.countplot(\"label\", data=train_data)\n",
    "# plt.title(\"train_data\")\n",
    "# plt.show()\n",
    "\n",
    "# 获得验证数据标签的数量分布\n",
    "# sns.countplot(\"label\", data=valid_data)\n",
    "# plt.title(\"valid_data\")\n",
    "# plt.show()\n",
    "\n",
    "# 在训练数据中添加新的句子长度列, 每个元素的值都是对应句子的长度\n",
    "train_data[\"sentence_length\"] = list(map(lambda x: len(x), train_data[\"sentence\"]))\n",
    "\n",
    "# 绘制句子长度列的数量分布\n",
    "# sns.countplot(\"sentence_length\", data=train_data)\n",
    "# plt.xticks([])\n",
    "# plt.show()\n",
    "\n",
    "# 绘制dist长度分布图\n",
    "# sns.distplot(train_data[\"sentence_length\"])\n",
    "# plt.yticks([])\n",
    "# plt.show()\n",
    "\n",
    "# 在验证数据中添加新的句子长度列, 每个元素的值对应句子的长度\n",
    "valid_data[\"sentence_length\"] = list(map(lambda x: len(x), valid_data[\"sentence\"]))\n",
    "\n",
    "# 绘制句子长度列的数量分布图\n",
    "# sns.countplot(\"sentence_length\", data=valid_data)\n",
    "# plt.xticks([])\n",
    "# plt.show()\n",
    "\n",
    "# 绘制dist长度分布图\n",
    "# sns.distplot(valid_data[\"sentence_length\"])\n",
    "# plt.yticks([])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# 绘制训练数据语句长度的散点图\n",
    "# sns.stripplot(y=\"sentence_length\", x=\"label\", data=train_data)\n",
    "# plt.show()\n",
    "\n",
    "# 绘制验证数据语句长度的散点图\n",
    "# sns.stripplot(y=\"sentence_length\", x=\"label\", data=valid_data)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# 导入jieba 工具包和chain工具包, 用于分词扁平化列表\n",
    "from itertools import chain\n",
    "\n",
    "# 进行训练集的句子进行分词, 并统计出不同词汇的总数\n",
    "# train_vocab = set(chain(*map(lambda x: jieba.lcut(x), train_data[\"sentence\"])))\n",
    "# print(\"训练集共包含不同词汇总数为:\", len(train_vocab))\n",
    "\n",
    "# 进行验证集的句子进行分词, 并统计出不同词汇的总数\n",
    "# valid_vocab = set(chain(*map(lambda x: jieba.lcut(x), valid_data[\"sentence\"])))\n",
    "# print(\"验证集共包含不同词汇总数为:\", len(valid_vocab))\n",
    "\n",
    "\n",
    "# 导入jieba 中的词性标注工具包\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "# 定义获取形容词的列表函数\n",
    "def get_a_list(text):\n",
    "    # 使用jieba的词性标注方法来切分文本, 获得两个属性word,flag\n",
    "    # 利用flag属性去判断一个词汇是否是形容词\n",
    "    r = []\n",
    "    for g in pseg.lcut(text):\n",
    "        if g.flag == 'a':\n",
    "            r.append(g.word)\n",
    "    return r\n",
    "\n",
    "\n",
    "# 导入绘制词云的工具包\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 定义获取词云的函数并绘图\n",
    "def get_word_cloud(keywords_list):\n",
    "    # 首先实例化词云类对象, 里面三个参数\n",
    "    # font_path: 字体路径,为了能够更好的显示中文\n",
    "    # max_words: 指定词云图像最多可以显示的词汇数量\n",
    "    # backgroud_color: 代表图片的北京颜色\n",
    "    wordcloud = WordCloud(max_words=100, background_color='white')\n",
    "\n",
    "    # 将传入的列表参数转化为字符串形式, 因为词云对象的参数要求是字符串类型\n",
    "    keywords_string = \" \".join(keywords_list)\n",
    "    # 生成词云\n",
    "    wordcloud.generate(keywords_string)\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 获取训练集上的正样本\n",
    "# p_train_data = train_data[train_data[\"label\"]==1][\"sentence\"]\n",
    "\n",
    "# 对正样本的每个句子提取形容词\n",
    "# train_p_a_vocab = chain(*map(lambda x: get_a_list(x), p_train_data))\n",
    "\n",
    "# 获取训练集上的负样本\n",
    "# n_train_data = train_data[train_data[\"label\"]==0][\"sentence\"]\n",
    "\n",
    "# 对负样本的每个句子提取形容词\n",
    "# train_n_a_vocab = chain(*map(lambda x: get_a_list(x), n_train_data))\n",
    "\n",
    "# 调用获取词云的函数\n",
    "# get_word_cloud(train_p_a_vocab)\n",
    "# get_word_cloud(train_n_a_vocab)\n",
    "\n",
    "\n",
    "# 获取验证集的数据正样本\n",
    "p_valid_data = valid_data[valid_data[\"label\"]==1][\"sentence\"]\n",
    "\n",
    "# 获取正样本的每个句子的形容词\n",
    "valid_p_a_vocab = chain(*map(lambda x: get_a_list(x), p_valid_data))\n",
    "\n",
    "# 获取验证集的数据负样本\n",
    "n_valid_data = valid_data[valid_data[\"label\"]==0][\"sentence\"]\n",
    "\n",
    "# 获取负样本的每个句子的形容词\n",
    "valid_n_a_vocab = chain(*map(lambda x: get_a_list(x), n_valid_data))\n",
    "\n",
    "# 调用获取词云的函数\n",
    "get_word_cloud(valid_p_a_vocab)\n",
    "get_word_cloud(valid_n_a_vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
