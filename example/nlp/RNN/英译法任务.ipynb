{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from io import open\n",
    "# 用于字符规范化\n",
    "import unicodedata\n",
    "# 用于正则表达式\n",
    "import re\n",
    "# 用于随机生成数据\n",
    "import random\n",
    "# 用于构建网络结构和函数的torch工具包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# torch中预定义的优化方法工具包\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:08.512779Z",
     "end_time": "2023-07-03T19:25:10.395534Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 对持久化文件中的数据进行处理，以满足模型训练的要求"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:14.140185Z",
     "end_time": "2023-07-03T19:25:14.161077Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 将指定语言中的词汇映射成数值\n",
    "# 起始标志\n",
    "SOS_token = 0\n",
    "# 结束标志\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        \"\"\"初始化函数中参数name代表传入某种语言的名字\"\"\"\n",
    "        # 将name传入类中\n",
    "        self.name = name\n",
    "        # 初始化词汇对应自然数值的字典\n",
    "        self.word2index = {}\n",
    "        # 初始化词汇对应自然数值的字典,其中0,1对应的SOS和EOS已经在里面了\n",
    "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "        # 初始化词汇对应的自然数索引，这里从2开始，因为0,1已经被开始和结束标志占用了\n",
    "        self.n_words = 2\n",
    "\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"添加句子函数，即将句子转化为对应的数值序列，输入参数sentence是一条句子\"\"\"\n",
    "        # 根据一般国家的语言特性(我们这里研究的语言都是以空格分隔单词)\n",
    "        # 对句子进行分割，得到对应的词汇列表\n",
    "        for word in sentence.split(' '):\n",
    "            # 然后调用addWord进行处理\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        \"\"\"添加词汇函数，即将词汇转化为对应的数值，输入参数word是一个单词\"\"\"\n",
    "        # 首先判断word是否已经在self.word2index字典的key中\n",
    "        if word not in self.word2index:\n",
    "            # 如果不在，则将这个词加入其中，并为它对应一个数值，即self.n_words\n",
    "            self.word2index[word] = self.n_words\n",
    "            # 同时也将它的反转形式加入到self.index2word中\n",
    "            self.index2word[self.n_words] = word\n",
    "            # self.n_words一旦被占用之后逐次加1,变成新的self.n_words\n",
    "            self.n_words += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:15.649998Z",
     "end_time": "2023-07-03T19:25:15.670615Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2index: {'Hello': 2, 'I': 3, 'am': 4, 'Jay': 5}\n",
      "index2word: {0: 'SOS', 1: 'EOS', 2: 'Hello', 3: 'I', 4: 'am', 5: 'Jay'}\n",
      "n_words: 6\n"
     ]
    }
   ],
   "source": [
    "# 实例化参数\n",
    "name = \"eng\"\n",
    "# 输入参数\n",
    "sentence = \"Hello I am Jay\"\n",
    "\n",
    "# 调用\n",
    "eng1  = Lang(name)\n",
    "eng1.addSentence(sentence)\n",
    "print(\"word2index:\", eng1.word2index)\n",
    "print(\"index2word:\", eng1.index2word)\n",
    "print(\"n_words:\", eng1.n_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:18.269829Z",
     "end_time": "2023-07-03T19:25:18.302547Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you kidding me ?\n"
     ]
    }
   ],
   "source": [
    "# 字符规范化\n",
    "# 将unicode转为Ascii,我们可以认为是去掉一些语言中的重音标记\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"字符串规范化函数，参数s代表传入的字符串\"\"\"\n",
    "    # 使字符变为小写工去除两侧空白符，z再使用unicodeToAsccii去掉重音标记\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # 在.!?前加一个空格\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # 使用正则表达式将字符串中不是大小写字母和正常标点的都替换成空格\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "# 输入参数\n",
    "s = \"Are you kidding me?\"\n",
    "\n",
    "# 调用\n",
    "nsr = normalizeString(s)\n",
    "print(nsr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:21.172367Z",
     "end_time": "2023-07-03T19:25:21.184803Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 将持久化文件中的数据加载到内存，并实例化类Lang\n",
    "data_path = './data/eng-fra.txt'\n",
    "\n",
    "def readLangs(lang1, lang2):\n",
    "    \"\"\"读取语言函数，参数lang1是源语言的名字，参数lang2是目标语言的名字， 返回对应的class Lang对象，以及语言对列表\"\"\"\n",
    "    # 从文件中读取语言对并以/n划分存到列表lines中\n",
    "    lines = open(data_path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # 对lines列表中的句子进行标准化处理，并以\\t进行再次划分，形成子列表，也就是语言对\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    # 然后分别将语言名字传入Lang类中，获得对应的语言对象，返回结果\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    return  input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:24.814247Z",
     "end_time": "2023-07-03T19:25:24.839953Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_lang: <__main__.Lang object at 0x7fb3d16ca2e0>\n",
      "output_lang: <__main__.Lang object at 0x7fb3d5332070>\n",
      "parirs中的前五个: [['go .', 'va !'], ['run !', 'cours !'], ['run !', 'courez !'], ['wow !', 'ca alors !'], ['fire !', 'au feu !']]\n"
     ]
    }
   ],
   "source": [
    "# 输入参数\n",
    "lang1 = \"eng\"\n",
    "lang2 = \"fra\"\n",
    "\n",
    "# 调用\n",
    "input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "print(\"input_lang:\", input_lang)\n",
    "print(\"output_lang:\", output_lang)\n",
    "print(\"parirs中的前五个:\", pairs[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:28.952254Z",
     "end_time": "2023-07-03T19:25:34.095368Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后的pairs前五个: [['i m .', 'j ai ans .'], ['i m ok .', 'je vais bien .'], ['i m ok .', 'ca va .'], ['i m fat .', 'je suis gras .'], ['i m fat .', 'je suis gros .']]\n"
     ]
    }
   ],
   "source": [
    "# 过滤出我们需要的语言对\n",
    "# 设置组成句子中单词或标点的最多个数\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# 选择带有指定前缀的语言特征数据作为训练数据\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    \"\"\"语言对过滤函数，参数p代表输入的语言对，如['she is afraid.', 'elle malade.']\"\"\"\n",
    "    # p[0]代表英语句子，对它进行划分，它的长度应小于最大长度MAX_LENGTH并且要以指定的前缀开头\n",
    "    # p[1]代表法文句子，对它进行划分，它的长度应小于最大长度MAX_LENGTH\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes) and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    \"\"\"对多个语言对列表进行过滤，参数pairs代表语言对组成的列表，简称语言对列表\"\"\"\n",
    "    # 函数中直接遍历列表中的每个语言对并调用filterPair即可\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# 输入参数paris使用readLangs函数的输出结果pairs\n",
    "# 调用\n",
    "fpairs = filterPairs(pairs)\n",
    "print(\"过滤后的pairs前五个:\", fpairs[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:36.680028Z",
     "end_time": "2023-07-03T19:25:36.736807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 对以上数据准备函数进行整合，并使用类Lang对语言对进行数据映射\n",
    "def prepareData(lang1, lang2):\n",
    "    \"\"\"数据准备函数，完成将所有字符串数据向数值型数据的映射以及过滤语言对参数lang1, lang2分别代表源语言和目标语言的名字\"\"\"\n",
    "    # 首先通过readLangs函数获得input_lang, output_lang对象，以及字符串类型的语言对列表\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "    # 对字符串类型的语言对列表进行过滤操作\n",
    "    pairs = filterPairs(pairs)\n",
    "    # 对过滤后的语言对列表进行遍历\n",
    "    for pair in pairs:\n",
    "        # 并使用input_lang和output_lang的addSentence方法对其进行数值映射\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    # 返回数值映射后的对象，和过滤后语言对\n",
    "    return  input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:40.267512Z",
     "end_time": "2023-07-03T19:25:40.295576Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 调用\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:43.630682Z",
     "end_time": "2023-07-03T19:25:49.040967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_n_words: 2803\n",
      "output_n_words: 4345\n",
      "['i m sorry you re leaving us .', 'c est triste que tu doives partir .']\n"
     ]
    }
   ],
   "source": [
    "print(\"input_n_words:\", input_lang.n_words)\n",
    "print(\"output_n_words:\", output_lang.n_words)\n",
    "print(random.choice(pairs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:49.043449Z",
     "end_time": "2023-07-03T19:25:49.052483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [1]]), tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1]]))\n"
     ]
    }
   ],
   "source": [
    "# 将语言对转化为模型输入需要的张量\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    \"\"\"将文本句子转换为张量，参数lang代表传入的Lang的实例化对象，sentence是预转换的句子\"\"\"\n",
    "    # 对句子进行分割并遍历每一个词汇，然后使用lang的word2index方法找到它对应的索引\n",
    "    # 这样就得到了该句子对应的数值列表\n",
    "    indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    # 然后加入句子结束标志\n",
    "    indexes.append(EOS_token)\n",
    "    # 将其使用torch.tensor封装成张量，并改变它的形状为nx1,以方便后续计算\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1,1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    \"\"\"将语言对转换为张量对，参数pair为一个语言对\"\"\"\n",
    "    # 调用tensorFromSentence分别将源语言和目标语言分别处理，获得对应的张量表示\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    # 最后返回它们组成的元组\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "# 输入参数，取pairs的第一条\n",
    "pair = pairs[0]\n",
    "\n",
    "# 调用\n",
    "pair_tensor = tensorFromPair(pair)\n",
    "print(pair_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:49.058894Z",
     "end_time": "2023-07-03T19:25:49.098447Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第三步：构建基于GRU的编码器和解码器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0759,  0.2893, -0.4880, -0.1749, -0.3697, -0.2214, -0.3071,\n",
      "          -0.2766,  0.3184, -0.1032,  0.1865,  0.2051, -0.2579, -0.0905,\n",
      "          -0.1794,  0.2911,  0.1269,  0.0689,  0.2692, -0.0983,  0.1131,\n",
      "          -0.0073, -0.2844, -0.2351,  0.2266]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"它初始化参数有两个，input_size代表解码器的输入尺寸即源语言的词表大小，hidden_size\n",
    "        代表GRU的隐藏层节点数，也代表词嵌入维度，同时又是GRU的输入尺寸\"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # 将参数hidden_size传入类中\n",
    "        self.hidden_size = hidden_size\n",
    "        # 实例化nn中预定义的Embedding层，它的参数分别是input_size,hidden_size\n",
    "        # 这里人词嵌入维度即hidden_size\n",
    "        # nn.Embedding的演示在该代码下方\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # 然后实例化nn中预定义的GRU层，它的参数是hidden_size\n",
    "        # nn.GRU的演示在该代码下方\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"编码器前身逻辑函数中参数有两个，input代表源语言的Embedding层输入张量 hidden代表编码器层gru的初始隐层张量\"\"\"\n",
    "        # 将输入张量进行embedding操作，并使其形状变为(1,1,-1),-1代表自动计算维度\n",
    "        # 理论上，我们的编码器每次只以一个词作为输入，因此词汇映射后的尺寸应该是[1, embedding]\n",
    "        # 而这里转换成三维的原因是因为torch中预定义gru必须使用三维张量作为输入，因此我们拓展了一个维度\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # 然后将embedding层的输出和传入的初始化hidden作为gru的输入传入其中\n",
    "        # 获得最终gru的输出output和对应的隐层张量hidden,并返回结果\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"初始化隐层张量函数\"\"\"\n",
    "        # 将隐层张量初始化成1x1xself.hidden_size大小的0张量\n",
    "        return  torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "# 实例化参数\n",
    "hidden_size = 25\n",
    "input_size = 20\n",
    "\n",
    "# 输入参数\n",
    "# pair_tensor[0]代表源语言即英文的句子，pair_tensor[0][0]代表句子中的第一个词\n",
    "input = pair_tensor[0][0]\n",
    "# 初始化第一个隐层张量，1x1xhidden_size的0张量\n",
    "hidden = torch.zeros(1, 1, hidden_size)\n",
    "\n",
    "# 调用\n",
    "encoder = EncoderRNN(input_size, hidden_size)\n",
    "encoder_output, hidden = encoder(input, hidden)\n",
    "print(encoder_output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:54.082663Z",
     "end_time": "2023-07-03T19:25:54.144316Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        \"\"\"初始化函数有两个参数，hidden_size代表解码器中的GRU输入尺寸，也是它的隐层节点数\n",
    "        output_size代表整个解码器的输出尺寸，也是我们希望的指定尺寸即目标语言的词表大小\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # 将hidden_size传入到类中\n",
    "        self.hidden_size = hidden_size\n",
    "        # 实例化一个nn中的embedding层对象，它的参数output这里表示目标语言的词表大小\n",
    "        # hidden_size表示目标语言的词嵌入维度\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # 实例化GRU对象，输入参数都是hidden_size,代表它的输入尺寸和隐层节点数相同\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # 实例化线性层，对GRU的输出做线性变化，获得我们希望的输出尺寸output_size\n",
    "        # 因此它的两个参数分别是hidden_size,output_size\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        # 最后使用softmax进行处理，以便于分类\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"解码器的前向逻辑函数中，参数有两个，input代表目标语言的Embedding层输入张量 hidden代表解码器GRU的初始隐层张量\"\"\"\n",
    "        # 将输入张量进行embedding操作，并使其形状变为(1, 1, -1)，-1代表自动计算维度\n",
    "        # 原因和解码器相同，因为torch预定义的GRU层只接受三维张量作为输入\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # 然后使用relu函数对输出进行处理，根据relu函数的特性，将使Embedding矩阵更稀疏，以防止\n",
    "        output = F.relu(output)\n",
    "        # 接下来，将把embedding的输出以及初始化的hidden张量传入到解码器gru中\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # 因为GRU输出的output也是三维张量，第一维没有意义，因此可以通过output[0]来降维\n",
    "        # 再传给线性层做变换，最后用softmax处理以便于分类\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"初始化隐层张量函数\"\"\"\n",
    "        # 将隐层张量初始化成为1x1xself.hidden_size大小的0张量\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:25:57.854522Z",
     "end_time": "2023-07-03T19:25:57.884663Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5320, -2.4627, -2.1612, -2.4296, -2.4357, -2.1388, -2.0293, -2.1476,\n",
      "         -2.3790, -2.4549]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 实例化参数\n",
    "hidden_size = 25\n",
    "output_size = 10\n",
    "\n",
    "# 输入参数\n",
    "# pair_tensor[1]代表目标语言即法文的句子，pair_tensor[1][0]代表句子中的第一个词\n",
    "input = pair_tensor[1][0]\n",
    "# 初始化第一个隐层张量 1x1xhidden_size的0张量\n",
    "hidden = torch.zeros(1, 1, hidden_size)\n",
    "\n",
    "# 调用\n",
    "decoder = DecoderRNN(hidden_size, output_size)\n",
    "output, hidden = decoder(input, hidden)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:26:00.921383Z",
     "end_time": "2023-07-03T19:26:00.952124Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"/Users/zhangli/Library/Application Support/typora-user-images/image-20230703173240587.png\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        \"\"\"\n",
    "        :param hidden_size: 代表解码器中GRU的输入尺寸，也是它的隐层节点数\n",
    "        :param output_size: 代表整个解码器的输出尺寸，也是我们希望得到的指定尺寸即目标语言的词表大小\n",
    "        :param dropout_p:代表我们使用dropout层时的置0率，默认为0.1\n",
    "        :param max_length: 代表句子的最大长度\n",
    "        \"\"\"\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # 将以下参数传入类中\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # 实例化一个Embedding层，输入参数是self.output_size(目标语言的词汇总数)和self.hidden_size(和词嵌入的维度)\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # 根据attention的QKV理论，attention的输入参数为三个Q、K、V\n",
    "        # 第一步：使用Q和K进行attention权值计算得到权重矩阵，再与V做矩阵乘法，得到V的注意力表示结果\n",
    "        # 这里常见的计算方式有三种\n",
    "        # 1. 将Q、K进行纵轴拼接，做一次线性变化，再使用softmax处理获得结果最后与V做张量乘法\n",
    "        # 2. 将Q、K进行纵轴拼接，做一次线性变化,再使用tanh函数，然后再进行内部求和，最后使用softmax处理获得结果再与V做张量乘法\n",
    "        # 3. 将Q和K的转置做点积运算，然后除以一个缩放系数，再使用softmax处理获得结果最后与V做张量乘法\n",
    "\n",
    "        # 说明：当注意力权重矩阵和V都是三维张量且第一维代表为batch条数时，则做bmm运算\n",
    "        # 第二步，根据第一步采用的计算方法，如果是拼接方法，则需要将Q与第二步的计算结果再进行拼接\n",
    "        # 如果是转置点积，一般是自注意力，Q与V相同，则不需要进行与Q的拼接，因此第二步的计算方式与第一步采用全值计算方法有关\n",
    "        # 第三步，最后为了使整个attention结构按照指定尺寸输出，使用线性层作用在第二步的结果上做一个线性变换，得到最终对Q的注意力表示\n",
    "\n",
    "        # 我们这里使用的是第一步中的第一种计算方式，因此需要一个线性变换的矩阵，实例化nn.Linear\n",
    "        # 因为它的输入是Q、K拼接，所以输入的第一个参数是self.hidden_size * 2,第二个参数是self.max_length\n",
    "        # 这里的Q是解码器的Embedding层的输出，K是解码器GRU的隐层输出，因为首次隐层还没有任何输出，会使用编码器的隐层输出\n",
    "        # 而这里的V是解码器层的输出\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        # 接着我们实例化另外一个线性层，它是attention理论中的第四步的线性层，用于规范输出尺寸\n",
    "        # 这里它的输入来自第三步的结果，因为第三步的结果是将Q与第二步的结果进行拼接，因此输入的维度是self.hidden_size * 2\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        # 接着实例化一个nn.Dropout层，并传入self.dropout_p\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        # 之后实例化nn.GRU,它的输入和隐层的尺寸都是self.hidden_size\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        # 最后实例化gru后面的线性层，也就是我们的解码器输出层\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"forward函数的输入参数有三个，分别是源数据输入张量，初始的隐层张量，以及解码器的输出张量\"\"\"\n",
    "        # 根据结构计算图，输入张量进行Embedding层并扩展维度\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # 使用dropout进行随机丢弃，防止过拟合\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # 进行attention的权重计算，我们使用第一种方式\n",
    "        # 将Q和K进行纵轴拼接，做一性线性变化，最后使用softmax处理获得结果\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # 然后进行第一步的后半部分，将得到的权重矩阵与V做矩阵乘法计算，当二者都是三维张量且第一维\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                  encoder_outputs.unsqueeze(0))\n",
    "        # 之后进行第二步，通过取[0]是用来降维，根据第一步采用的计算方法，需要将Q与第一步的计算结果\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "\n",
    "        # 最后是第三步，使用线性层作用在第三步的结果上做一个线性变换并扩展维度，得到输出\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        # attention结构的结果使用relu激活\n",
    "        output = F.relu(output)\n",
    "\n",
    "        # 将激活后的结果作为gru的输入和hideen一起传入其中\n",
    "        output. hidden = self.gru(output, hidden)\n",
    "\n",
    "        # 最后将结果降维并使用softmax处理得到最终的结果\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        # 返回解码器结果，最后隐层张量以及注意力权重张量\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"\n",
    "        初始化隐层张量为1x1xself.hidden_size大小的张量\n",
    "        :param self:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return  torch.zeros(1, 1, self.hidden_size, device = device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:27:06.001988Z",
     "end_time": "2023-07-03T19:27:06.027987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0.]]])\n",
      "tensor([[-2.4642, -2.9059, -2.3662, -2.3890, -1.7398, -2.5366, -2.2330, -2.5282,\n",
      "         -2.0142, -2.3074]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 1, 25])\n",
      "tensor([[0.1039, 0.0769, 0.2070, 0.0836, 0.0378, 0.0322, 0.0350, 0.1376, 0.1054,\n",
      "         0.1807]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 实例化参数\n",
    "hidden_size = 25\n",
    "output_size = 10\n",
    "\n",
    "# 输入参数\n",
    "input = pair_tensor[1][0]\n",
    "print(input)\n",
    "hidden = torch.zeros(1, 1, hidden_size)\n",
    "print(hidden)\n",
    "# encoder_outputs需要是encoder中每一个时间步的输出堆叠而成\n",
    "# 它的形状应该是10x25,我们这里直接随机初始化一个张量\n",
    "encoder_outputs  = torch.randn(10, 25)\n",
    "\n",
    "# 调用\n",
    "decoder = AttnDecoderRNN(hidden_size, output_size)\n",
    "output, hidden, attn_weights = decoder(input, hidden, encoder_outputs)\n",
    "print(output)\n",
    "print(output.shape)\n",
    "print(hidden.shape)\n",
    "print(attn_weights)\n",
    "print(attn_weights.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-03T19:56:36.956288Z",
     "end_time": "2023-07-03T19:56:36.974325Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第四步：构建模型训练函数，并进行训练\n",
    "- 什么是teacher_forcing?\n",
    "它是一种用于序列生成任务的训练技巧，在seq2seq架构中，根据循环神经网络理论，解码器每次应该使用上一步的结果作为输入的一部分，但是在训练的过程中，一旦上一步的结果是错误的，就会导致这种错误被累积，无法达到训练效果，因此，我们需要一种机制改变上一步出错的情况，因为训练时，我们是已知正确的输出应该是什么的，因此可以强制将上一步结果设置为正确的输出 ，这种方式就叫做teacher_forcing\n",
    "- teacher_forcing的作用：\n",
    "    - 能够在训练的时候矫正模型的预测，避免在序列生成的过程中误差进一步放大\n",
    "    - teacher_forcing能够极大的加快模型的收敛速度，令模型训练过程更快更平稳"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 设置teacher_forcing比率为0.5\n",
    "teeacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, encoder_optimizer, decoder_optimizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
