{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"/Users/zhangli/Library/Application Support/typora-user-images/image-20230615172857911.png\">\n",
    "\n",
    "##### seq2seq模型架构分析\n",
    "\n",
    "- seq2seq模型分成两部分，分别是encoder(编码器)和decoder(解码器)，编码器和解码器的内部实现都使用GRU模型，这里它要完成的是一个中文到英文的翻译 欢迎 来 北京 --> welcome to BeiJing.编码器首先处理中文输入\"欢迎 来 北京\",通过GRU模型获取每个时间步的输出张量，最后将它们拼接成一个中间语义张量c，接着解码器将使用这个中间语义张量c以及每一个时间步的隐层张量，逐个生成对的翻译语言\n",
    "\n",
    "##### 基于GRU的seq2seq模型架构实现翻译的过程\n",
    "\n",
    "- 第一步：导入必备工具包\n",
    "- 第二步：对持久化文件中的数据进行处理，以满足模型训练需求\n",
    "- 第三步：构建基于GRU的编码器和解码器\n",
    "- 第四步：构建模型训练函数，并进行训练\n",
    "- 第五步：构建模型评估函数，并进行测试以及Attention效果分析\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from io import open\n",
    "# 用于字符规范化\n",
    "import unicodedata\n",
    "# 用于正则表达式\n",
    "import re\n",
    "# 用于随机生成数据\n",
    "import random\n",
    "# 用于构建网络结构和函数的torch工具包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# torch中预定义的优化方法工具包\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T10:26:12.918003251Z",
     "start_time": "2023-07-02T10:26:11.947076940Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 对持久化文件中的数据进行处理，以满足模型训练的要求"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T10:26:12.929278711Z",
     "start_time": "2023-07-02T10:26:12.922744960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# 将指定语言中的词汇映射成数值\n",
    "# 起始标志\n",
    "SOS_token = 0\n",
    "# 结束标志\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        \"\"\"初始化函数中参数name代表传入某种语言的名字\"\"\"\n",
    "        # 将name传入类中\n",
    "        self.name = name\n",
    "        # 初始化词汇对应自然数值的字典\n",
    "        self.word2index = {}\n",
    "        # 初始化词汇对应自然数值的字典,其中0,1对应的SOS和EOS已经在里面了\n",
    "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "        # 初始化词汇对应的自然数索引，这里从2开始，因为0,1已经被开始和结束标志占用了\n",
    "        self.n_words = 2\n",
    "\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"添加句子函数，即将句子转化为对应的数值序列，输入参数sentence是一条句子\"\"\"\n",
    "        # 根据一般国家的语言特性(我们这里研究的语言都是以空格分隔单词)\n",
    "        # 对句子进行分割，得到对应的词汇列表\n",
    "        for word in sentence.split(' '):\n",
    "            # 然后调用addWord进行处理\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        \"\"\"添加词汇函数，即将词汇转化为对应的数值，输入参数word是一个单词\"\"\"\n",
    "        # 首先判断word是否已经在self.word2index字典的key中\n",
    "        if word not in self.word2index:\n",
    "            # 如果不在，则将这个词加入其中，并为它对应一个数值，即self.n_words\n",
    "            self.word2index[word] = self.n_words\n",
    "            # 同时也将它的反转形式加入到self.index2word中\n",
    "            self.index2word[self.n_words] = word\n",
    "            # self.n_words一旦被占用之后逐次加1,变成新的self.n_words\n",
    "            self.n_words += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T10:26:11.945178537Z",
     "start_time": "2023-07-02T10:26:11.937836581Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2index: {'Hello': 2, 'I': 3, 'am': 4, 'Jay': 5}\n",
      "index2word: {0: 'SOS', 1: 'EOS', 2: 'Hello', 3: 'I', 4: 'am', 5: 'Jay'}\n",
      "n_words: 6\n"
     ]
    }
   ],
   "source": [
    "# 实例化参数\n",
    "name = \"eng\"\n",
    "# 输入参数\n",
    "sentence = \"Hello I am Jay\"\n",
    "\n",
    "# 调用\n",
    "eng1  = Lang(name)\n",
    "eng1.addSentence(sentence)\n",
    "print(\"word2index:\", eng1.word2index)\n",
    "print(\"index2word:\", eng1.index2word)\n",
    "print(\"n_words:\", eng1.n_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T10:26:13.041906079Z",
     "start_time": "2023-07-02T10:26:12.934520634Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are you kidding me ?\n"
     ]
    }
   ],
   "source": [
    "# 字符规范化\n",
    "# 将unicode转为Ascii,我们可以认为是去掉一些语言中的重音标记\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"字符串规范化函数，参数s代表传入的字符串\"\"\"\n",
    "    # 使字符变为小写工去除两侧空白符，z再使用unicodeToAsccii去掉重音标记\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # 在.!?前加一个空格\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # 使用正则表达式将字符串中不是大小写字母和正常标点的都替换成空格\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "# 输入参数\n",
    "s = \"Are you kidding me?\"\n",
    "\n",
    "# 调用\n",
    "nsr = normalizeString(s)\n",
    "print(nsr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T10:37:09.808261971Z",
     "start_time": "2023-07-02T10:37:09.724559515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 将持久化文件中的数据加载到内存，并实例化类Lang\n",
    "data_path = './data/eng-fra.txt'\n",
    "\n",
    "def readLangs(lang1, lang2):\n",
    "    \"\"\"读取语言函数，参数lang1是源语言的名字，参数lang2是目标语言的名字， 返回对应的class Lang对象，以及语言对列表\"\"\"\n",
    "    # 从文件中读取语言对并以/n划分存到列表lines中\n",
    "    lines = open(data_path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # 对lines列表中的句子进行标准化处理，并以\\t进行再次划分，形成子列表，也就是语言对\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    # 然后分别将语言名字传入Lang类中，获得对应的语言对象，返回结果\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    return  input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T10:41:54.633380438Z",
     "start_time": "2023-07-02T10:41:54.585480813Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_lang: <__main__.Lang object at 0x7f1d4d569d00>\n",
      "output_lang: <__main__.Lang object at 0x7f1d4d54e910>\n",
      "parirs中的前五个: [['go .', 'va !'], ['run !', 'cours !'], ['run !', 'courez !'], ['wow !', 'ca alors !'], ['fire !', 'au feu !']]\n"
     ]
    }
   ],
   "source": [
    "# 输入参数\n",
    "lang1 = \"eng\"\n",
    "lang2 = \"fra\"\n",
    "\n",
    "# 调用\n",
    "input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "print(\"input_lang:\", input_lang)\n",
    "print(\"output_lang:\", output_lang)\n",
    "print(\"parirs中的前五个:\", pairs[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T10:43:35.059618363Z",
     "start_time": "2023-07-02T10:43:28.671494429Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后的pairs前五个: [['i m .', 'j ai ans .'], ['i m ok .', 'je vais bien .'], ['i m ok .', 'ca va .'], ['i m fat .', 'je suis gras .'], ['i m fat .', 'je suis gros .']]\n"
     ]
    }
   ],
   "source": [
    "# 过滤出我们需要的语言对\n",
    "# 设置组成句子中单词或标点的最多个数\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# 选择带有指定前缀的语言特征数据作为训练数据\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    \"\"\"语言对过滤函数，参数p代表输入的语言对，如['she is afraid.', 'elle malade.']\"\"\"\n",
    "    # p[0]代表英语句子，对它进行划分，它的长度应小于最大长度MAX_LENGTH并且要以指定的前缀开头\n",
    "    # p[1]代表法文句子，对它进行划分，它的长度应小于最大长度MAX_LENGTH\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes) and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    \"\"\"对多个语言对列表进行过滤，参数pairs代表语言对组成的列表，简称语言对列表\"\"\"\n",
    "    # 函数中直接遍历列表中的每个语言对并调用filterPair即可\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# 输入参数paris使用readLangs函数的输出结果pairs\n",
    "# 调用\n",
    "fpairs = filterPairs(pairs)\n",
    "print(\"过滤后的pairs前五个:\", fpairs[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T10:57:59.536571094Z",
     "start_time": "2023-07-02T10:57:59.419441402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 对以上数据准备函数进行整合，并使用类Lang对语言对进行数据映射\n",
    "def prepareData(lang1, lang2):\n",
    "    \"\"\"数据准备函数，完成将所有字符串数据向数值型数据的映射以及过滤语言对参数lang1, lang2分别代表源语言和目标语言的名字\"\"\"\n",
    "    # 首先通过readLangs函数获得input_lang, output_lang对象，以及字符串类型的语言对列表\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "    # 对字符串类型的语言对列表进行过滤操作\n",
    "    pairs = filterPairs(pairs)\n",
    "    # 对过滤后的语言对列表进行遍历\n",
    "    for pair in pairs:\n",
    "        # 并使用input_lang和output_lang的addSentence方法对其进行数值映射\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    # 返回数值映射后的对象，和过滤后语言对\n",
    "    return  input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:02:55.087662713Z",
     "start_time": "2023-07-02T11:02:55.021967636Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 调用\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:03:22.343101639Z",
     "start_time": "2023-07-02T11:03:16.688994620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_n_words: 2803\n",
      "output_n_words: 4345\n",
      "['we re not serious .', 'nous ne sommes pas serieux .']\n"
     ]
    }
   ],
   "source": [
    "print(\"input_n_words:\", input_lang.n_words)\n",
    "print(\"output_n_words:\", output_lang.n_words)\n",
    "print(random.choice(pairs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:03:59.333430526Z",
     "start_time": "2023-07-02T11:03:59.298240972Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [1]]), tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1]]))\n"
     ]
    }
   ],
   "source": [
    "# 将语言对转化为模型输入需要的张量\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    \"\"\"将文本句子转换为张量，参数lang代表传入的Lang的实例化对象，sentence是预转换的句子\"\"\"\n",
    "    # 对句子进行分割并遍历每一个词汇，然后使用lang的word2index方法找到它对应的索引\n",
    "    # 这样就得到了该句子对应的数值列表\n",
    "    indexes = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    # 然后加入句子结束标志\n",
    "    indexes.append(EOS_token)\n",
    "    # 将其使用torch.tensor封装成张量，并改变它的形状为nx1,以方便后续计算\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1,1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    \"\"\"将语言对转换为张量对，参数pair为一个语言对\"\"\"\n",
    "    # 调用tensorFromSentence分别将源语言和目标语言分别处理，获得对应的张量表示\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    # 最后返回它们组成的元组\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "# 输入参数，取pairs的第一条\n",
    "pair = pairs[0]\n",
    "\n",
    "# 调用\n",
    "pair_tensor = tensorFromPair(pair)\n",
    "print(pair_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:12:31.066950925Z",
     "start_time": "2023-07-02T11:12:31.026877932Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第三步：构建基于GRU的编码器和解码器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dddddd\n",
      "tensor([[[-0.1102,  0.0311, -0.0523,  0.0442, -0.0260, -0.2472,  0.2437,\n",
      "          -0.1598, -0.2752, -0.0652, -0.1625,  0.2190, -0.0734, -0.0828,\n",
      "          -0.1835, -0.3950, -0.0497,  0.1056,  0.2923, -0.1624, -0.0079,\n",
      "          -0.0828, -0.1002,  0.3109, -0.0750]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"它初始化参数有两个，input_size代表解码器的输入尺寸即源语言的词表大小，hidden_size\n",
    "        代表GRU的隐藏层节点数，也代表词嵌入维度，同时又是GRU的输入尺寸\"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # 将参数hidden_size传入类中\n",
    "        self.hidden_size = hidden_size\n",
    "        # 实例化nn中预定义的Embedding层，它的参数分别是input_size,hidden_size\n",
    "        # 这里人词嵌入维度即hidden_size\n",
    "        # nn.Embedding的演示在该代码下方\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # 然后实例化nn中预定义的GRU层，它的参数是hidden_size\n",
    "        # nn.GRU的演示在该代码下方\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"编码器前身逻辑函数中参数有两个，input代表源语言的Embedding层输入张量 hidden代表编码器层gru的初始隐层张量\"\"\"\n",
    "        # 将输入张量进行embedding操作，并使其形状变为(1,1,-1),-1代表自动计算维度\n",
    "        # 理论上，我们的编码器每次只以一个词作为输入，因此词汇映射后的尺寸应该是[1, embedding]\n",
    "        # 而这里转换成三维的原因是因为torch中预定义gru必须使用三维张量作为输入，因此我们拓展了一个维度\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # 然后将embedding层的输出和传入的初始化hidden作为gru的输入传入其中\n",
    "        # 获得最终gru的输出output和对应的隐层张量hidden,并返回结果\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"初始化隐层张量函数\"\"\"\n",
    "        # 将隐层张量初始化成1x1xself.hidden_size大小的0张量\n",
    "        return  torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "# 实例化参数\n",
    "hidden_size = 25\n",
    "input_size = 20\n",
    "\n",
    "# 输入参数\n",
    "# pair_tensor[0]代表源语言即英文的句子，pair_tensor[0][0]代表句子中的第一个词\n",
    "input = pair_tensor[0][0]\n",
    "# 初始化第一个隐层张量，1x1xhidden_size的0张量\n",
    "hidden = torch.zeros(1, 1, hidden_size)\n",
    "\n",
    "# 调用\n",
    "encoder = EncoderRNN(input_size, hidden_size)\n",
    "encoder_output, hidden = encoder(input, hidden)\n",
    "print(encoder_output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:38:10.232592762Z",
     "start_time": "2023-07-02T11:38:10.191506781Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        \"\"\"初始化函数有两个参数，hidden_size代表解码器中的GRU输入尺寸，也是它的隐层节点数\n",
    "        output_size代表整个解码器的输出尺寸，也是我们希望的指定尺寸即目标语言的词表大小\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # 将hidden_size传入到类中\n",
    "        self.hidden_size = hidden_size\n",
    "        # 实例化一个nn中的embedding层对象，它的参数output这里表示目标语言的词表大小\n",
    "        # hidden_size表示目标语言的词嵌入维度\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # 实例化GRU对象，输入参数都是hidden_size,代表它的输入尺寸和隐层节点数相同\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # 实例化线性层，对GRU的输出做线性变化，获得我们希望的输出尺寸output_size\n",
    "        # 因此它的两个参数分别是hidden_size,output_size\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        # 最后使用softmax进行处理，以便于分类\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"解码器的前向逻辑函数中，参数有两个，input代表目标语言的Embedding层输入张量 hidden代表解码器GRU的初始隐层张量\"\"\"\n",
    "        # 将输入张量进行embedding操作，并使其形状变为(1, 1, -1)，-1代表自动计算维度\n",
    "        # 原因和解码器相同，因为torch预定义的GRU层只接受三维张量作为输入\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # 然后使用relu函数对输出进行处理，根据relu函数的特性，将使Embedding矩阵更稀疏，以防止\n",
    "        output = F.relu(output)\n",
    "        # 接下来，将把embedding的输出以及初始化的hidden张量传入到解码器gru中\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # 因为GRU输出的output也是三维张量，第一维没有意义，因此可以通过output[0]来降维\n",
    "        # 再传给线性层做变换，最后用softmax处理以便于分类\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"初始化隐层张量函数\"\"\"\n",
    "        # 将隐层张量初始化成为1x1xself.hidden_size大小的0张量\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:54:48.566779162Z",
     "start_time": "2023-07-02T11:54:48.530041140Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5834, -2.0119, -2.3289, -2.3253, -2.3923, -2.3584, -2.0943, -2.1243,\n",
      "         -2.4600, -2.5071]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 实例化参数\n",
    "hidden_size = 25\n",
    "output_size = 10\n",
    "\n",
    "# 输入参数\n",
    "# pair_tensor[1]代表目标语言即法文的句子，pair_tensor[1][0]代表句子中的第一个词\n",
    "input = pair_tensor[1][0]\n",
    "# 初始化第一个隐层张量 1x1xhidden_size的0张量\n",
    "hidden = torch.zeros(1, 1, hidden_size)\n",
    "\n",
    "# 调用\n",
    "decoder = DecoderRNN(hidden_size, output_size)\n",
    "output, hidden = decoder(input, hidden)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T11:57:05.645412642Z",
     "start_time": "2023-07-02T11:57:05.580781573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
