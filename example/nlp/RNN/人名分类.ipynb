{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 人名分类问题：\n",
    "- 以一个人名为输入，使用模型帮助我们判断它最有可能是来自哪一个国家的人名，这在某些国际化公司的业务中具有重要意义，在用户注册过程中，会根据用户填写的名字直接给他分配可能的国家或地区选项，以及该国家或地区的国旗，限制手机号吗位数等等\n",
    "\n",
    "## 整个案例分为5个步骤\n",
    "- 第一步：导入必要的工具包\n",
    "- 第二步: 对data文件中的数据进行处理，满足训练要求\n",
    "- 第三步: 构建RNN模型(包括传统的RNN,LSTM以及GRU等)\n",
    "- 第四步：构建训练函数并进行训练\n",
    "- 第五步：构建评估函数并进行预测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_letter: 57\n",
      "all_letters: abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n"
     ]
    }
   ],
   "source": [
    "#  第一步：导入必要的工具包\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import random\n",
    "import time\n",
    "import match\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 第二步：对data文件中的数据进行处理，满足训练要求\n",
    "# 获取所有的常用字符包括字母和常用标点\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "# 获取常用字符数量\n",
    "n_letters = len(all_letters)\n",
    "print(\"n_letter:\", n_letters)\n",
    "print(\"all_letters:\", all_letters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T15:40:04.226256Z",
     "end_time": "2023-06-14T15:40:04.238561Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 字符规范之unicode转Ascii函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "# 关于编码问题我们暂且不去考虑\n",
    "# 这个函数的作用就是去掉一些语言中的重音标记\n",
    "# 如: Ślusàrski ---> Slusarski\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "# 调用\n",
    "s = \"Ślusàrski\"\n",
    "a = unicodeToAscii(s)\n",
    "print(a)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T14:08:46.824421Z",
     "end_time": "2023-06-14T14:08:46.829214Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ang', 'AuYong', 'Bai', 'Ban', 'Bao', 'Bei', 'Bian', 'Bui', 'Cai', 'Cao', 'Cen', 'Chai', 'Chaim', 'Chan', 'Chang', 'Chao', 'Che', 'Chen', 'Cheng', 'Cheung', 'Chew', 'Chieu', 'Chin', 'Chong', 'Chou', 'Chu', 'Cui', 'Dai', 'Deng', 'Ding', 'Dong', 'Dou', 'Duan', 'Eng', 'Fan', 'Fei', 'Feng', 'Foong', 'Fung', 'Gan', 'Gauk', 'Geng', 'Gim', 'Gok', 'Gong', 'Guan', 'Guang', 'Guo', 'Gwock', 'Han', 'Hang', 'Hao', 'Hew', 'Hiu', 'Hong', 'Hor', 'Hsiao', 'Hua', 'Huan', 'Huang', 'Hui', 'Huie', 'Huo', 'Jia', 'Jiang', 'Jin', 'Jing', 'Joe', 'Kang', 'Kau', 'Khoo', 'Khu', 'Kong', 'Koo', 'Kwan', 'Kwei', 'Kwong', 'Lai', 'Lam', 'Lang', 'Lau', 'Law', 'Lew', 'Lian', 'Liao', 'Lim', 'Lin', 'Ling', 'Liu', 'Loh', 'Long', 'Loong', 'Luo', 'Mah', 'Mai', 'Mak', 'Mao', 'Mar', 'Mei', 'Meng', 'Miao', 'Min', 'Ming', 'Moy', 'Mui', 'Nie', 'Niu', 'OuYang', 'OwYang', 'Pan', 'Pang', 'Pei', 'Peng', 'Ping', 'Qian', 'Qin', 'Qiu', 'Quan', 'Que', 'Ran', 'Rao', 'Rong', 'Ruan', 'Sam', 'Seah', 'See ', 'Seow', 'Seto', 'Sha', 'Shan', 'Shang', 'Shao', 'Shaw', 'She', 'Shen', 'Sheng', 'Shi', 'Shu', 'Shuai', 'Shui', 'Shum', 'Siew', 'Siu', 'Song', 'Sum', 'Sun', 'Sze ', 'Tan', 'Tang', 'Tao', 'Teng', 'Teoh', 'Thean', 'Thian', 'Thien', 'Tian', 'Tong', 'Tow', 'Tsang', 'Tse', 'Tsen', 'Tso', 'Tze', 'Wan', 'Wang', 'Wei', 'Wen', 'Weng', 'Won', 'Wong', 'Woo', 'Xiang', 'Xiao', 'Xie', 'Xing', 'Xue', 'Xun', 'Yan', 'Yang', 'Yao', 'Yap', 'Yau', 'Yee', 'Yep', 'Yim', 'Yin', 'Ying', 'Yong', 'You', 'Yuan', 'Zang', 'Zeng', 'Zha', 'Zhan', 'Zhang', 'Zhao', 'Zhen', 'Zheng', 'Zhong', 'Zhou', 'Zhu', 'Zhuo', 'Zong', 'Zou', 'Bing', 'Chi', 'Chu', 'Cong', 'Cuan', 'Dan', 'Fei', 'Feng', 'Gai', 'Gao', 'Gou', 'Guan', 'Gui', 'Guo', 'Hong', 'Hou', 'Huan', 'Jian', 'Jiao', 'Jin', 'Jiu', 'Juan', 'Jue', 'Kan', 'Kuai', 'Kuang', 'Kui', 'Lao', 'Liang', 'Lu', 'Luo', 'Man', 'Nao', 'Pian', 'Qiao', 'Qing', 'Qiu', 'Rang', 'Rui', 'She', 'Shi', 'Shuo', 'Sui', 'Tai', 'Wan', 'Wei', 'Xian', 'Xie', 'Xin', 'Xing', 'Xiong', 'Xuan', 'Yan', 'Yin', 'Ying', 'Yuan', 'Yue', 'Yun', 'Zha', 'Zhai', 'Zhang', 'Zhi', 'Zhuan', 'Zhui']\n"
     ]
    }
   ],
   "source": [
    "# 构建一个从持久化文件中读取内容到内存的函数\n",
    "data_path = \"./data/names/\"\n",
    "\n",
    "def readLines(filename):\n",
    "    \"\"\"从文件中读取每一行加载到内存中形成列表\"\"\"\n",
    "    # 打开指定文件并读取所有内容, 使用strip()去除两侧空白符, 然后以'\\n'进行切分\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    # 对应每一个lines列表中的名字进行Ascii转换, 使其规范化.最后返回一个名字列表\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# filename是数据集中某个具体的文件, 我们这里选择Chinese.txt\n",
    "filename = data_path + \"Chinese.txt\"\n",
    "lines = readLines(filename)\n",
    "print(lines)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T14:12:28.330307Z",
     "end_time": "2023-06-14T14:12:28.355682Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 构建人名类别(所属的语言)列表和人名对应关系字典"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_categories: 18\n"
     ]
    }
   ],
   "source": [
    "# 构建的category_lines形如：{\"English\":[\"Lily\", \"Susan\", \"Kobe\"], \"Chinese\":[\"Zhang San\", \"Xiao Ming\"]}\n",
    "category_lines = {}\n",
    "# all_categories形如： [\"English\",...,\"Chinese\"]\n",
    "all_categories = []\n",
    "\n",
    "# 读取指定路径下的txt文件，使用glob,path中可以使用正则表达式\n",
    "for filename in glob.glob(data_path + \"*.txt\"):\n",
    "    # 获取每个文件的文件名，就是对应的名字类别\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    # 将其逐一加到all_categories列表中\n",
    "    all_categories.append(category)\n",
    "    # 然后读取每个文件的内容，形成名字列表\n",
    "    lines = readLines(filename)\n",
    "    # 按照对应的类别，将名字列表写入到catagory_lines字典中\n",
    "    category_lines[category] = lines\n",
    "\n",
    "# 查看类别总数\n",
    "n_categories = len(all_categories)\n",
    "print(\"n_categories:\", n_categories)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T14:28:52.056103Z",
     "end_time": "2023-06-14T14:28:52.125734Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "# 随便查看其中的一些内容\n",
    "print(category_lines['Italian'][:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T14:34:02.880920Z",
     "end_time": "2023-06-14T14:34:02.903555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line_tensor: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 将人名转化为对应的one-hot张量表示\n",
    "# 将字符串(单词粒度)转化为张量表示，如：\"ab\" --->\n",
    "# tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "#        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0.]]])\n",
    "def lineToTensor(line):\n",
    "    \"\"\"将人名转化为对应的one-hot张量表示，参数line是输入的人名\"\"\"\n",
    "    # 首先初始化一个0张量，它的形状(len(line), 1, n_letters)\n",
    "    # 代表人名中的每个字母用一个1 x n_letters的张量表示\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    # 遍历这个人名中的每个字符索引和字符\n",
    "    for li, letter in enumerate(line):\n",
    "        # 使用字符串方法find找到每个字符在all_letters中的索引\n",
    "        # 它也是我们生成one-hot张量中1的索引位置\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    # 返回结果\n",
    "    return  tensor\n",
    "\n",
    "\n",
    "line = \"Bai\"\n",
    "line_tensor = lineToTensor(line)\n",
    "print(\"line_tensor:\", line_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T14:41:10.920248Z",
     "end_time": "2023-06-14T14:41:10.989334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 构建传统RNN模型\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        \"\"\"初始化函数中有4个参数，分别代表RNN输入最后一维尺寸，RNN隐层最后一维尺寸,RNN层数\"\"\"\n",
    "        # input_size:代表RNN输入的最后一个维度\n",
    "        # hidden_size:代表RNN隐藏层的最后一个维度\n",
    "        # output_size:代表RNN网络最后线性层的输出维度\n",
    "        # num_layers:代表RNN网络的层数\n",
    "        super(RNN, self).__init__()\n",
    "        # 将hidden_size与num_layers传入其中\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 实例化预定义的nn.RNN,它的三个参数分别是input_size，hidden_size, num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers)\n",
    "        # 实例化nn.Linear,这个线性层用于将nn.RNN的输出值维度转化为指定的输出维度\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        # 实例化nn中预定的Softmax层，用于从输出层获得类别结果\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"完成传统RNN的主要逻辑，输入参数input代表输入张量，它的形状是1 x n_letters\n",
    "        hidden代表RNN的隐层张量，它的形状是self.num_layers x 1 x self.hidden_size\"\"\"\n",
    "        # 因此预定义的nn.RNN要求输入维度一定是三维张量，因此在这里使用unsqueeze(0)扩展一个维度\n",
    "        input = input.unsqueeze(0)\n",
    "        # 将input和hidden输入到传统RNN的实例化对象中，如果num_layers = 1，rr恒等于hn\n",
    "        rr, hn = self.rnn(input, hidden)\n",
    "        # 将从RNN中获得的结果通过线性变换和softmax返回，同时返回hn作为后续RNN的输入\n",
    "        return self.softmax(self.linear(rr)), hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        \"\"\"初始化隐层张量\"\"\"\n",
    "        # 初始化一个(self.num_layers, 1, self.hidden_size)形状为0的张量\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T14:59:03.735388Z",
     "end_time": "2023-06-14T14:59:03.789147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze演示\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(x.shape)\n",
    "z = torch.unsqueeze(x, 0)\n",
    "print(z.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T17:25:19.145953Z",
     "end_time": "2023-06-14T17:25:19.170802Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.unsqueeze(x, 1)\n",
    "print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T17:26:01.114235Z",
     "end_time": "2023-06-14T17:26:01.128334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 构建LSTM模型\n",
    "# 使用nn.LSTM构建完成LSTM使用类\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        # 初始化函数的参数与传统RNN相同\n",
    "        super(LSTM, self).__init__()\n",
    "        # 将hidden_size与num_layers传入其中\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 实例化预定义的nn.LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        # 实例化nn.Linear,这个线性层用于将nn.LSTM的输出维度转化为指定的输出维度\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        # 实例化nn中预定的Softmax层，用于从输出层获得类别结果\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, hidden, c):\n",
    "        \"\"\"在主要逻辑函数中多出一个参数c, 也就是LSTM中的细胞状态张量\"\"\"\n",
    "        # 使用unsqueeze(0)扩展一个维度\n",
    "        input = input.unsqueeze(0)\n",
    "        # 将input, hidden以及初始化的c传入lstm中\n",
    "        rr, (hn, c) = self.lstm(input, (hidden, c))\n",
    "        # 最后返回处理后rr, hn, c\n",
    "        return self.softmax(self.linear(rr)), hn, c\n",
    "\n",
    "    def initHiddenAndC(self):\n",
    "        \"\"\"初始化函数不仅初始化hidden还要初始化细胞状态c，它们的形状相同\"\"\"\n",
    "        c = hidden = torch.zeros(self.num_layers, 1, self.hidden_size)\n",
    "        return  hidden, c"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T15:13:45.863570Z",
     "end_time": "2023-06-14T15:13:45.907037Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 构建GRU模型\n",
    "# 使用nn.GRU构建完成传统RNN使用类\n",
    "\n",
    "# GRU与传统RNN的外部形式相同, 都是只传递隐层张量, 因此只需要更改预定义层的名字\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 实例化预定义的nn.GRU, 它的三个参数分别是input_size, hidden_size, num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(0)\n",
    "        rr, hn = self.gru(input, hidden)\n",
    "        return self.softmax(self.linear(rr)), hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T15:31:44.877276Z",
     "end_time": "2023-06-14T15:31:44.934377Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 实例化参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 因为是onehot编码, 输入张量最后一维的尺寸就是n_letters\n",
    "input_size = n_letters\n",
    "\n",
    "# 定义隐层的最后一维尺寸大小\n",
    "n_hidden = 57\n",
    "\n",
    "# 输出尺寸为语言类别总数n_categories\n",
    "output_size = n_categories\n",
    "\n",
    "# num_layer使用默认值, num_layers = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T18:44:14.344939Z",
     "end_time": "2023-06-14T18:44:14.367777Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 输入参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# 假如我们以一个字母B作为RNN的首次输入, 它通过lineToTensor转为张量\n",
    "# 因为我们的lineToTensor输出是三维张量, 而RNN类需要的二维张量\n",
    "# 因此需要使用squeeze(0)降低一个维度\n",
    "input = lineToTensor('B').squeeze(0)\n",
    "\n",
    "# 初始化一个三维的隐层0张量, 也是初始的细胞状态张量\n",
    "hidden = c = torch.zeros(1, 1, n_hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T18:44:18.812224Z",
     "end_time": "2023-06-14T18:44:18.832205Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 调用"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn: tensor([[[-2.9879, -3.1140, -2.9302, -2.8892, -3.0818, -2.7007, -2.6251,\n",
      "          -2.8833, -2.8417, -2.9852, -2.9925, -3.0889, -2.9286, -2.7733,\n",
      "          -2.7649, -2.8631, -2.9344, -2.7964]]], grad_fn=<LogSoftmaxBackward0>)\n",
      "rnn.shape: torch.Size([1, 1, 18])\n",
      "lstm: tensor([[[-2.9077, -2.7731, -2.9073, -2.9322, -2.9242, -2.9533, -2.8012,\n",
      "          -2.7922, -2.7910, -2.8518, -2.9046, -3.0164, -2.9827, -2.9458,\n",
      "          -2.9559, -2.7684, -2.8808, -2.9921]]], grad_fn=<LogSoftmaxBackward0>)\n",
      "lstm.shape: torch.Size([1, 1, 18])\n",
      "gru: tensor([[[-2.7727, -2.7887, -2.9656, -2.9033, -2.9723, -2.9675, -2.8865,\n",
      "          -2.7472, -2.9438, -2.8640, -3.0225, -2.8290, -3.0852, -2.9195,\n",
      "          -2.8837, -2.7212, -2.9279, -2.9047]]], grad_fn=<LogSoftmaxBackward0>)\n",
      "gru.shape: torch.Size([1, 1, 18])\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "lstm = LSTM(n_letters, n_hidden, n_categories)\n",
    "gru = GRU(n_letters, n_hidden, n_categories)\n",
    "\n",
    "rnn_output, next_hidden = rnn(input, hidden)\n",
    "print(\"rnn:\", rnn_output)\n",
    "print(\"rnn.shape:\", rnn_output.shape)\n",
    "lstm_output, next_hidden, c = lstm(input, hidden, c)\n",
    "print(\"lstm:\", lstm_output)\n",
    "print(\"lstm.shape:\", lstm_output.shape)\n",
    "gru_output, next_hidden = gru(input, hidden)\n",
    "print(\"gru:\", gru_output)\n",
    "print(\"gru.shape:\", gru_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T18:49:38.923266Z",
     "end_time": "2023-06-14T18:49:38.944040Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 第四步：构建训练函数并进行训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# 从输出结果中获得指定类别的函数\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    \"\"\"从输出结果中获得指定类别，参数为输出张量output\"\"\"\n",
    "    # 从输出张量中返回最大的值和索引，作为我们的类别信息，我们这里主要需要这个索引\n",
    "    top_n, top_i = output.topk(1)\n",
    "    # top_i对象中取出索引的值\n",
    "    category_i = top_i[0].item()\n",
    "    # 根据索引获得对应语言类型，返回语言类别和索引值\n",
    "    return all_categories[category_i], category_i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T18:54:00.358717Z",
     "end_time": "2023-06-14T18:54:00.396493Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n",
      "torch.return_types.topk(\n",
      "values=tensor([5., 4., 3.]),\n",
      "indices=tensor([4, 3, 2]))\n"
     ]
    }
   ],
   "source": [
    "# torch.topk演示\n",
    "x = torch.arange(1., 6.)\n",
    "print(x)\n",
    "z = torch.topk(x, 3)\n",
    "print(z)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T18:55:14.566272Z",
     "end_time": "2023-06-14T18:55:14.586877Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: Dutch\n",
      "category_i: 15\n"
     ]
    }
   ],
   "source": [
    "# 输入参数\n",
    "# 将上一步中grur的输出作为函数的输入\n",
    "output = gru_output\n",
    "# 调用\n",
    "category, category_i = categoryFromOutput(output)\n",
    "print(\"category:\", category)\n",
    "print(\"category_i:\", category_i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T18:56:51.909796Z",
     "end_time": "2023-06-14T18:56:51.960995Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 随机生成训练数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def randomTrainingExample():\n",
    "    \"\"\"该函数用于随机产生训练数据\"\"\"\n",
    "    # 首先使用random的choice方法从all_categories随机选择一个类别\n",
    "    category = random.choice(all_categories)\n",
    "    # 然后再通过category_lines字典取category类别对应的名字列表\n",
    "    # 之后再从列表中随机取一个名字\n",
    "    line = random.choice(category_lines[category])\n",
    "    # 接着将这个类别在所有类别列表中的索引封装成tensor,得到类别张量category_tensor\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    # 最后，将随机取到的名字通过函数lineToTensor转化为onehot张量表示\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T19:10:12.230994Z",
     "end_time": "2023-06-14T19:10:12.279676Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Arabic / line = Haddad / category_tensor = tensor([2])\n",
      "category = Spanish / line = Rosa / category_tensor = tensor([10])\n",
      "category = Portuguese / line = Crespo / category_tensor = tensor([13])\n",
      "category = Vietnamese / line = Quyen / category_tensor = tensor([5])\n",
      "category = Italian / line = Palmisano / category_tensor = tensor([12])\n",
      "category = Dutch / line = Rompaeij / category_tensor = tensor([15])\n",
      "category = Greek / line = Chrysanthopoulos / category_tensor = tensor([11])\n",
      "category = Polish / line = Rudawski / category_tensor = tensor([17])\n",
      "category = Russian / line = Abelsky / category_tensor = tensor([6])\n",
      "category = Spanish / line = Morales / category_tensor = tensor([10])\n"
     ]
    }
   ],
   "source": [
    "# 我们随机取出十个进行结果查看\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line, '/ category_tensor =', category_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T19:10:15.657652Z",
     "end_time": "2023-06-14T19:10:15.755557Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 构建传统RNN训练函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# 定义损失函数为nn.NLLLoss，因为RNN的最后一层是nn.LogSoftmax,两者的内部计算逻辑正好能够吻合\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# 设置学习率\n",
    "learning_rate = 0.005\n",
    "\n",
    "def trainRNN(category_tensor, line_tensor):\n",
    "    \"\"\"定义训练函数，它的两个参数是category_tensor类别的张量表示，相当于训练数据的标签\n",
    "    line_tensor名字张量的表示，相当于对应训练数据\"\"\"\n",
    "    # 在函数中，首先通过实例化对象rnn初始化隐层张量\n",
    "    hidden = rnn.initHidden()\n",
    "    # 然后将模型结构中的梯度归0\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # 下面开始进行训练，将训练数据line_tensor的每个字符逐个传入rnn之中，得到最终结果\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    # 因为我们的rnn对象由nn.RNN实例化得到，最终输出形状是三维张量，为了满足于category_tensor\n",
    "    # 进行对比计算损失, 需要减少第一个维度, 这里使用squeeze()方法\n",
    "    loss = criterion(output.squeeze(0), category_tensor)\n",
    "\n",
    "    # 损失进行反向传播\n",
    "    loss.backward()\n",
    "    # 更新模型中所有的参数\n",
    "    for p in rnn.parameters():\n",
    "        # 将参数的张量表示与参数的梯度乘以学习率的结果相加以此来更新参数\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "    # 返回结果和损失的值\n",
    "    return output, loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-14T19:26:28.555803Z",
     "end_time": "2023-06-14T19:26:28.592303Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
