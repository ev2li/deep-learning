{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### 文本预处理及作用:\n",
    "\n",
    "- 文本语料在输送给模型前一般需要一系列的预处理工作，才能符合模型输入的要求，如:将文本转化成模型需要的张量，规范张量的尺寸等，而科学的文本预处理环节将有效指导模型超参数的选择，提升模型的评估指标\n",
    "\n",
    "#### 文本处理中包含的主要环节:\n",
    "\n",
    "- 文本处理的基本方法\n",
    "- 文本张量表示方法\n",
    "- 文本语料的数据分析\n",
    "- 文本特征处理\n",
    "- 数据增强方法\n",
    "\n",
    "#### 文本处理的基本方法\n",
    "\n",
    "- 分词\n",
    "- 词性标注\n",
    "- 命名实体识别\n",
    "\n",
    "#### 文本张量表示方法\n",
    "\n",
    "- one-hot编码\n",
    "- Word2vec\n",
    "- Word Embedding\n",
    "\n",
    "#### 文本语料的数据分析\n",
    "\n",
    "- 标签数量分布\n",
    "- 句子长度分布\n",
    "- 词频统计与关键词词云\n",
    "\n",
    "#### 文本特征处理\n",
    "\n",
    "- 添加n-gram特征\n",
    "- 文本长度规范\n",
    "\n",
    "#### 数据增加方法\n",
    "\n",
    "- 回译数据增加法\n",
    "\n",
    "#### 重要说明:\n",
    "\n",
    "- 在实际生产应用中，我们最常使用的两种语言是中文和英文，因此，文本预处理部分的内容都将针对这两种语言进行讲解"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/2g/d27fqh0s1x15sknzqypt6cd00000gn/T/jieba.cache\n",
      "Loading model cost 0.722 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": "['工信处',\n '女干事',\n '每月',\n '经过',\n '下属',\n '科室',\n '都',\n '要',\n '亲口',\n '交代',\n '24',\n '口',\n '交换机',\n '等',\n '技术性',\n '器件',\n '的',\n '安装',\n '工作']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "content = \"工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作\"\n",
    "jieba.cut(content, cut_all=False) # cut_all默认是False\n",
    "\n",
    "# 将返回一个生成器对象\n",
    "\n",
    "# 若需直接返回列表内容，使用jieba.lcut即可\n",
    "jieba.lcut(content, cut_all=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T11:05:17.279663Z",
     "end_time": "2023-06-08T11:05:18.054596Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['工信处',\n '处女',\n '女干事',\n '干事',\n '每月',\n '月经',\n '经过',\n '下属',\n '科室',\n '都',\n '要',\n '亲口',\n '口交',\n '交代',\n '24',\n '口交',\n '交换',\n '交换机',\n '换机',\n '等',\n '技术',\n '技术性',\n '性器',\n '器件',\n '的',\n '安装',\n '安装工',\n '装工',\n '工作']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全模型分词\n",
    "    # 把句子中所有的可以成词的词语都扫描出来\n",
    "jieba.cut(content, cut_all=True)\n",
    "jieba.lcut(content, cut_all=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T11:07:36.468255Z",
     "end_time": "2023-06-08T11:07:36.481396Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 搜索引擎模式分词\n",
    "- 在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object Tokenizer.cut_for_search at 0x7f92f10d6c70>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.cut_for_search(content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T11:09:49.767205Z",
     "end_time": "2023-06-08T11:09:49.797788Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['工信处',\n '干事',\n '女干事',\n '每月',\n '经过',\n '下属',\n '科室',\n '都',\n '要',\n '亲口',\n '交代',\n '24',\n '口',\n '交换',\n '换机',\n '交换机',\n '等',\n '技术',\n '技术性',\n '器件',\n '的',\n '安装',\n '工作']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut_for_search(content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T11:09:58.605452Z",
     "end_time": "2023-06-08T11:09:58.642230Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 中文繁体分词\n",
    "- 针对中国香港，台湾地区的繁体文本进行分词"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 添加自定义词典后，jiebaa能够益友识别词典中出现的词汇，提升整体识别准确率\n",
    "#### 词典格式:每一行分为三部分:词语、词频(可省略)、词性(可省略)，用空格隔开，顺序不可颠倒"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['八', '一双', '鹿', '更名', '为', '八一', '南昌', '篮球队', '!']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut(\"八一双鹿更名为八一南昌篮球队!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T11:18:34.194543Z",
     "end_time": "2023-06-08T11:18:34.222539Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['八一双鹿', '更名', '为', '八一', '南昌', '篮球队', '!']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.load_userdict(\"./userdict.txt\")\n",
    "jieba.lcut(\"八一双鹿更名为八一南昌篮球队!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T11:19:16.441636Z",
     "end_time": "2023-06-08T11:19:16.453064Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 流行的中英文分词工具hanlp\n",
    "- 中英文NLP处理工具包，基于tensorflow2.0使用在学术界和行业中推广最先进的深度学习技术"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 188.3 MiB   2.4 MiB/s ETA:  0 s [=========================================]\n",
      "Decompressing /Users/zhangli/.hanlp/embeddings/convseg_embeddings.zip to /Users/zhangli/.hanlp/embeddings\n",
      "Loading word2vec from cache \u001B[5m\u001B[33m...\u001B[0m\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00%    0 KB/14.9 MB Loading word2vec from text file \u001B[5m\u001B[33m...\u001B[0m\u001B[0m ETA: 22 h 31 m 58 s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r          Downloading https://file.hankcs.com/corpus/char_table.zip#CharTable.txt to /Users/zhangli/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.zip\n",
      "100%  17.6 KiB  17.6 KiB/s ETA:  0 s [=========================================]\n",
      "Decompressing /Users/zhangli/.hanlp/thirdparty/file.hankcs.com/corpus/char_table.zip to /Users/zhangli/.hanlp/thirdparty/file.hankcs.com/corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": "['工信处',\n '女',\n '干事',\n '每',\n '月',\n '经过',\n '下属',\n '科室',\n '都',\n '要',\n '亲口',\n '交代',\n '24',\n '口',\n '交换机',\n '等',\n '技术性',\n '器件',\n '的',\n '安装',\n '工作']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "['工信处',\n '女',\n '干事',\n '每',\n '月',\n '经过',\n '下属',\n '科室',\n '都',\n '要',\n '亲口',\n '交代',\n '24',\n '口',\n '交换机',\n '等',\n '技术性',\n '器件',\n '的',\n '安装',\n '工作']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hanlp\n",
    "# 加载CTB_CONVSEG预训练模型进行分词任务\n",
    "tokenizer = hanlp.load('CTB6_CONVSEG')\n",
    "tokenizer(\"工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T14:00:50.302050Z",
     "end_time": "2023-06-08T14:00:50.987755Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'hanlp.utils.rules' has no attribute 'tokenize_english'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 进行英文分词，英文分词只需要使用规则即可\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mhanlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrules\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize_english\u001B[49m\n\u001B[1;32m      3\u001B[0m tokenizer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMr. Hankcs bought hankcs.com for 1.5 thousand dollars\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'hanlp.utils.rules' has no attribute 'tokenize_english'"
     ]
    }
   ],
   "source": [
    "# 进行英文分词，英文分词只需要使用规则即可\n",
    "tokenizer = hanlp.utils.rules.tokenize_english\n",
    "tokenizer('Mr. Hankcs bought hankcs.com for 1.5 thousand dollars')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 什么是命名实体识别\n",
    "\n",
    "- 命名实体：通常我们将人名、地名、机构名、等专有名词统称命名实体.如：周杰伦、黑山县、孔子学院、24辊方钢矫直机\n",
    "- 顾名思义，命名实体识别(Named Entity Recongnition, 简称NER)就是识别出一段文本中可能存在的命名实体\n",
    "\n",
    "#### e.g.\n",
    "\n",
    "- 鲁迅，浙江绍兴人，五四新文化运动的重要参与者，代表作朝花夕拾\n",
    "- 鲁迅(人名)，浙江绍兴（地名）人，五四新文化运动（专有名词）的重要参与者，代表作朝花夕拾（专有名词）\n",
    "\n",
    "#### 命名实体的作用\n",
    "\n",
    "- 同词汇一样，命名实体也是人类理解文本的基础单元，因此也是AI解决NLP领域高阶任务的重要基础环节\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://file.hankcs.com/hanlp/ner/ner_bert_base_msra_20211227_114712.zip to /Users/zhangli/.hanlp/ner/ner_bert_base_msra_20211227_114712.zip\n",
      "100% 362.0 MiB   1.8 MiB/s ETA:  0 s [=========================================]\n",
      "Decompressing /Users/zhangli/.hanlp/ner/ner_bert_base_msra_20211227_114712.zip to /Users/zhangli/.hanlp/ner\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c61497074f141ea800b1f478d72ea25"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "663ed2d9b78e4ba78a241c3f5a5a75e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51626f11e1b54cefba8f694d913a2f12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ad769b4d2784d7183c6cd1995af8cef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tf_model.h5:   0%|          | 0.00/478M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06f99cee7e2e42a0ac8a683a173b0a37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[('上海华安工业(集团)公司', 'NT', 0, 12),\n ('谭旭光', 'NR', 15, 18),\n ('张晚霞', 'NR', 21, 24),\n ('美图', 'NS', 26, 28),\n ('纽约现代艺术博物馆', 'NS', 28, 37)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用hanlp进行中文命名实体识别\n",
    "# 加载中文命名实体识别的预训练模型MSRA_NER_BERT_BASE_ZH\n",
    "recongnizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)\n",
    "# 这里注意它的输入是对句子进行字符分割的列表，因此在句子前加入list()\n",
    "recongnizer(list('上海华安工业(集团)公司董事长谭旭光和秘书张晚霞来到美图纽约现代艺术博物馆参观'))\n",
    "# 返回结果是一个装有n个元组的列表,每个元组代表一个命名实体，元组中每一项分别代表具有命名实体"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T14:18:35.443786Z",
     "end_time": "2023-06-08T14:22:34.677587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Obama', 'PER', 1, 2), ('White House', 'LOC', 6, 8)]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用hanlp进行英文命名实体的识别\n",
    "# 加载英文命名实体识别的预训练模型CONLL03_NER_BERT_BASE_UNCASED_EH\n",
    "recongnizer = (hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_CASED_EN))\n",
    "# 这里注意它的输入是对句子进行分词后的结果，是列表形式\n",
    "recongnizer([\"President\", \"Obama\", \"is\", \"speaking\", \"at\", \"the\", \"White\", \"House\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T14:30:59.354128Z",
     "end_time": "2023-06-08T14:31:07.444450Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 词性标注\n",
    "\n",
    "- 词性：语言中对词的一种分类方法，以语法特征为主要依据，兼顾词汇意义对词进行划分的结果，常见的词性有14种，如：名词，动词，形容词等\n",
    "- 顾名思义：词性标注(Part-of-speech tagging，简称POS)就是标注出一段文本中每个词汇的词性\n",
    "\n",
    "#### e.g.\n",
    "\n",
    "- 我爱自然语言处理\n",
    "- 我/rr, 爱/v, 自然语言/n,处理/vn\n",
    "- rr:人称代词\n",
    "- v：动词\n",
    "- n: 名词\n",
    "- vn:动名词\n",
    "\n",
    "#### 词性标注的作用\n",
    "\n",
    "- 词性标注以分词为基础，是对文本语言的另一个角度的理解，因此也常常成为AI解决NLP领域高阶任务的重要基础环节\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[pair('我', 'r'), pair('爱', 'v'), pair('北京', 'ns'), pair('天安门', 'ns')]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用jieba进行中文词性标注\n",
    "import jieba.posseg as pseg\n",
    "pseg.lcut(\"我爱北京天安门\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T14:40:32.391578Z",
     "end_time": "2023-06-08T14:40:32.872648Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip#wiki.zh.bin to /Users/zhangli/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip\n",
      "100%   3.1 GiB 458.4 KiB/s ETA:  0 s [=========================================]\n",
      "Decompressing /Users/zhangli/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh.zip to /Users/zhangli/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.zh\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": "['PN', 'DEG', 'NN', 'VC', 'VV', 'NN']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用hanlp进行中文词性标注\n",
    "# 加载中文命名实体识别的预训练模型(CTB5_POS_RNN_FASTTEXT_ZH)\n",
    "tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)\n",
    "\n",
    "# 输入是分词结果列表\n",
    "tagger(['我', '的', '希望', '是', '希望', '和平'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-08T14:49:47.824909Z",
     "end_time": "2023-06-08T14:59:30.709996Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://file.hankcs.com/hanlp/pos/ptb_pos_rnn_fasttext_20220418_101708.zip to /Users/zhangli/.hanlp/pos/ptb_pos_rnn_fasttext_20220418_101708.zip\n",
      "100%   1.5 MiB   1.0 MiB/s ETA:  0 s [=========================================]\n",
      "Decompressing /Users/zhangli/.hanlp/pos/ptb_pos_rnn_fasttext_20220418_101708.zip to /Users/zhangli/.hanlp/pos\n",
      "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz to /Users/zhangli/.hanlp/thirdparty/dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
      "100%   4.2 GiB 474.1 KiB/s ETA:  0 s [=========================================]\n",
      "Failed to load https://file.hankcs.com/hanlp/pos/ptb_pos_rnn_fasttext_20220418_101708.zip\n",
      "If the problem still persists, please submit an issue to https://github.com/hankcs/HanLP/issues\n",
      "When reporting an issue, make sure to paste the FULL ERROR LOG below.\n",
      "================================ERROR LOG BEGINS================================\n",
      "OS: macOS-10.16-x86_64-i386-64bit\n",
      "Python: 3.10.11\n",
      "PyTorch: 1.13.1\n",
      "TensorFlow: 2.10.0\n",
      "HanLP: 2.1.0-beta.50\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 使用hanlp进行英文词性标注\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# 加载英文命名实体识别的预训练模型PTB_POS_RNN_FASTTEXT_EN\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m tagger \u001B[38;5;241m=\u001B[39m \u001B[43mhanlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhanlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpretrained\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPTB_POS_RNN_FASTTEXT_EN\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 输入是分词结果列表\u001B[39;00m\n\u001B[1;32m      5\u001B[0m tagger([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mI\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbanked\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdollars\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124min\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbank\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/__init__.py:43\u001B[0m, in \u001B[0;36mload\u001B[0;34m(save_dir, verbose, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhanlp_common\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstant\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HANLP_VERBOSE\n\u001B[1;32m     42\u001B[0m     verbose \u001B[38;5;241m=\u001B[39m HANLP_VERBOSE\n\u001B[0;32m---> 43\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_from_meta_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmeta.json\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/utils/component_util.py:186\u001B[0m, in \u001B[0;36mload_from_meta_file\u001B[0;34m(save_dir, meta_filename, transform_only, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m--> 186\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m e \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/utils/component_util.py:106\u001B[0m, in \u001B[0;36mload_from_meta_file\u001B[0;34m(save_dir, meta_filename, transform_only, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfig.json\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[0;32m--> 106\u001B[0m         \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    108\u001B[0m         obj\u001B[38;5;241m.\u001B[39mload(metapath, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/common/keras_component.py:215\u001B[0m, in \u001B[0;36mKerasComponent.load\u001B[0;34m(self, save_dir, logger, **kwargs)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload_config(save_dir)\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload_vocabs(save_dir)\n\u001B[0;32m--> 215\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmerge_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogger\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogger\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverwrite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload_weights(save_dir, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload_meta(save_dir)\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/common/keras_component.py:225\u001B[0m, in \u001B[0;36mKerasComponent.build\u001B[0;34m(self, logger, **kwargs)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuild\u001B[39m(\u001B[38;5;28mself\u001B[39m, logger, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform\u001B[38;5;241m.\u001B[39mbuild_config()\n\u001B[0;32m--> 225\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmerge_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtraining\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mloss\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform\u001B[38;5;241m.\u001B[39mlock_vocabs()\n\u001B[1;32m    228\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_optimizer(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig)\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/components/taggers/rnn_tagger_tf.py:35\u001B[0m, in \u001B[0;36mRNNTaggerTF.build_model\u001B[0;34m(self, embeddings, embedding_trainable, rnn_input_dropout, rnn_output_dropout, rnn_units, loss, **kwargs)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuild_model\u001B[39m(\u001B[38;5;28mself\u001B[39m, embeddings, embedding_trainable, rnn_input_dropout, rnn_output_dropout, rnn_units,\n\u001B[1;32m     32\u001B[0m                 loss,\n\u001B[1;32m     33\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mModel:\n\u001B[1;32m     34\u001B[0m     model \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mSequential()\n\u001B[0;32m---> 35\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_vocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m     model\u001B[38;5;241m.\u001B[39madd(embeddings)\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m rnn_input_dropout:\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/layers/embeddings/util_tf.py:44\u001B[0m, in \u001B[0;36mbuild_embedding\u001B[0;34m(embeddings, word_vocab, transform)\u001B[0m\n\u001B[1;32m     42\u001B[0m     embeddings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_vocab\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m transform\u001B[38;5;241m.\u001B[39mchar_vocab\n\u001B[1;32m     43\u001B[0m     transform\u001B[38;5;241m.\u001B[39mmap_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m---> 44\u001B[0m layer: tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mEmbedding \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeserialize_keras_object\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Embedding specific configuration\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFastTextEmbedding\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFastTextEmbeddingTF\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/keras/utils/generic_utils.py:744\u001B[0m, in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001B[0m\n\u001B[1;32m    742\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    743\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m CustomObjectScope(custom_objects):\n\u001B[0;32m--> 744\u001B[0m             deserialized_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcls_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    746\u001B[0m     \u001B[38;5;66;03m# Then `cls` may be a function returning a class.\u001B[39;00m\n\u001B[1;32m    747\u001B[0m     \u001B[38;5;66;03m# in this case by convention `config` holds\u001B[39;00m\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;66;03m# the kwargs of the function.\u001B[39;00m\n\u001B[1;32m    749\u001B[0m     custom_objects \u001B[38;5;241m=\u001B[39m custom_objects \u001B[38;5;129;01mor\u001B[39;00m {}\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/keras/engine/base_layer.py:828\u001B[0m, in \u001B[0;36mLayer.from_config\u001B[0;34m(cls, config)\u001B[0m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    813\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_config\u001B[39m(\u001B[38;5;28mcls\u001B[39m, config):\n\u001B[1;32m    814\u001B[0m     \u001B[38;5;124;03m\"\"\"Creates a layer from its config.\u001B[39;00m\n\u001B[1;32m    815\u001B[0m \n\u001B[1;32m    816\u001B[0m \u001B[38;5;124;03m    This method is the reverse of `get_config`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;124;03m        A layer instance.\u001B[39;00m\n\u001B[1;32m    827\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 828\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/layers/embeddings/fast_text_tf.py:24\u001B[0m, in \u001B[0;36mFastTextEmbeddingTF.__init__\u001B[0;34m(self, filepath, padding, name, **kwargs)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding \u001B[38;5;241m=\u001B[39m padding\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilepath \u001B[38;5;241m=\u001B[39m filepath\n\u001B[0;32m---> 24\u001B[0m filepath \u001B[38;5;241m=\u001B[39m \u001B[43mget_resource\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(filepath), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mResolved path \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a file\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     26\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoading fasttext model from [\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m].\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(filepath))\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/utils/io_util.py:341\u001B[0m, in \u001B[0;36mget_resource\u001B[0;34m(path, save_dir, extract, prefix, append_location, verbose)\u001B[0m\n\u001B[1;32m    339\u001B[0m         path \u001B[38;5;241m=\u001B[39m realpath\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m extract \u001B[38;5;129;01mand\u001B[39;00m compressed:\n\u001B[0;32m--> 341\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[43muncompress\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m anchor:\n\u001B[1;32m    343\u001B[0m         path \u001B[38;5;241m=\u001B[39m path_join(path, anchor)\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/utils/io_util.py:222\u001B[0m, in \u001B[0;36muncompress\u001B[0;34m(path, dest, remove, verbose)\u001B[0m\n\u001B[1;32m    220\u001B[0m         remove_file(prefix)\n\u001B[1;32m    221\u001B[0m         remove_file(path)\n\u001B[0;32m--> 222\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/site-packages/hanlp/utils/io_util.py:218\u001B[0m, in \u001B[0;36muncompress\u001B[0;34m(path, dest, remove, verbose)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m gzip\u001B[38;5;241m.\u001B[39mopen(path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f_in, \u001B[38;5;28mopen\u001B[39m(prefix, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f_out:\n\u001B[0;32m--> 218\u001B[0m         \u001B[43mshutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopyfileobj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf_in\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf_out\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    220\u001B[0m     remove_file(prefix)\n",
      "File \u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.10/shutil.py:198\u001B[0m, in \u001B[0;36mcopyfileobj\u001B[0;34m(fsrc, fdst, length)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 198\u001B[0m \u001B[43mfdst_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOSError\u001B[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# 使用hanlp进行英文词性标注\n",
    "# 加载英文命名实体识别的预训练模型PTB_POS_RNN_FASTTEXT_EN\n",
    "tagger = hanlp.load(hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN)\n",
    "# 输入是分词结果列表\n",
    "tagger(['I', 'banked', '2', 'dollars', 'in', 'a', 'bank', '.'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 小节总结\n",
    "\n",
    "- 学习什么是分词\n",
    "\t- 分词就是将连续的字序列按照一定的规范重新组合成词序列的过程，我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字，句和段能通过明显分界来简单划界，唯独没有一个形式上的分界符，分词就是找到这样分界符的过程\n",
    "- 学习分词的作用\n",
    "\t- 词作为语言语义理解的最小单元，是人类理解文本语言的基础，因此也是AI解决NLP领域高阶任务，如自动问答，机器翻译，文本生成的重要基础环节\n",
    "- 学习了流行中文分词工具jieba\n",
    "\t- 支持多种分词模式，精确模式，全模式，搜索引擎模式\n",
    "\t- 支持中文繁体分词\n",
    "\t- 支持用户自定义词典\n",
    "- 学习什么是命名实体识别\n",
    "\t- 命名实体:通常我们将人名，地名，机构名等专有名词统称命名实体，如：周杰伦，黑山县。孔子学院，24辊方钢矫直机\n",
    "\t- 顾名思义：命名实体识别(Named Entity Recognition,简称NER)就是识别出一段文件中可能存在的命名实体\n",
    "- 命名实体的作用\n",
    "\t- 同词汇一样，命名实体也是人类理解文本的基础单元，因此也是AI解决NLP领域高阶任务的重要基础环节\n",
    "- 词性标注\n",
    "\t- 词性：语言中对词的一种分类方法，以语法特征为主要依据，兼顾词汇意义对词进行划分的结果，常见的词性有14种，如：名词，动词，形容词等\n",
    "\t- 顾名思义：词性标注(Part-of-speech tagging，简称POS)就是标注出一段文本中每个词汇的词性\n",
    "- 词性标注的作用\n",
    "\t-  词性标注以分词为基础，是对文本语言的另一个角度的理解，因此也常常成为AI解决NLP领域高阶任务的重要基础环节"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
